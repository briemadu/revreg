{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Reading Time Corpora\n",
    "\n",
    "This notebook reads from each reading time corpora and generates ```.tsv``` files containing the data we need, i.e.:\n",
    "\n",
    "- ```Identifier```: a string containing the text ID and the token position in the text. The IDs may or not match the original ones: They are created in an arbitrary order here for our standard naming and can be mapped back to originals using the ```.json``` maps (see below). \n",
    "- ```Token```: a string containing the token as shown to the subjects (e.g. punctuation is considered together with the neighbor token). For some datasets we need some acrobacy to infer what exactly was shown in the screen. In general, it should be the text contained in the column Interest Area. When the raw texts are available, we check that the token order matches the texts.\n",
    "- ```<Measure>:Subj_i```: The measure for subject $i$ for each token. E.g. for first pass regression, a binary variable which is 1 if subject $i$ initiated a regression at the current token. Such column is created for each subject $i$. \n",
    "\n",
    "For standardization, we create a convention to use integers to refer to subjects and texts and also save the map from the ids to the original naming. We make text ids, subject ids and word positions start from 0. They can be mapped back to the original using the ```.json``` maps in the directory ```preprocessed/maps/``` and ```preprocessed/texts/```.\n",
    "\n",
    "### Decisions\n",
    "\n",
    "- We will extract first-pass regressions (i.e. we won't consider regressions that occurr in subsequent passes) and number of fixations.\n",
    "- For first-pass regressions, the value is ```0.0``` if no regression was initiated at that token in the first pass. If there is a regression from that token, the value is ```1.0```. For tokens skipped in the first pass reading, we use ```-1.0```.\n",
    "- Subjects with missing data are filled with ```NaN```.\n",
    "- Some clarification requests were sent for the authors of Provo, Rastros, Nicenboim. See meta.md for the replies.\n",
    "\n",
    "Tokens can be:\n",
    "1. Never fixated (therefore no regressions)\n",
    "2. Skipped at first pass but fixated later (regressions can happen, but we don't care for them here)\n",
    "3. Fixated at first pass (and thus regress / not regress)\n",
    "\n",
    "For us, case 1 and 2 are the same (word was skipped at first pass). We need to extract that from the data formats. If the word was skipped, we use the skipped label. If the word was not skipped, then we use the first-pass regression label.\n",
    "\n",
    "### Generated Files\n",
    "\n",
    "Running this notebook will create the following outputs:\n",
    "\n",
    "- ```preprocessed/human_data/```: one ```.tsv``` file for each measure and each dataset.\n",
    "- ```preprocessed/maps/```: two ```JSON``` files for each dataset. ```*__subjects``` maps original subject ids to integers and ```*_texts``` maps original text ids to integers.\n",
    "- ```preprocessed/texts/```: a ```JSON``` file for each dataset. One key for each text id (our integers). The values are dictionaries mapping token positions (from 0 to text length - 1) to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import rdata\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEXTS = Path('preprocessed/texts/')\n",
    "PATH_TO_HUMAN_DATA = Path('preprocessed/human_data/')\n",
    "PATH_TO_MAPS = Path('preprocessed/maps/')\n",
    "\n",
    "# columns and naming standards\n",
    "IDENTIFIER = 'Identifier'\n",
    "TOKEN = 'Token'\n",
    "VAR_SUBJ = '{}:Subj_{}'        # measure name and subject ID\n",
    "TOKEN_ID = 'text_{}_token_{}'  # text ID and token position in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide what value to use for words that were skipped and for really missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_LABEL = -1.0\n",
    "MISSING_LABEL = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stardandise the sometimes different measure namings as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Measure = namedtuple('Measure', ['name', 'shortform'])\n",
    "\n",
    "# Was there a regression initiating at a token in the first pass?\n",
    "FPREGOUT = Measure('first-pass-regression-out', 'fpregout')  # binary, or categorial if skips are identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(varname, measures, identifiers, ordered_subjects):\n",
    "    \"\"\"Return a dataframe in the standard format, given the data dictionaries.\"\"\"\n",
    "    columns = [IDENTIFIER, TOKEN] + [VAR_SUBJ.format(varname.shortform, subject) for subject in ordered_subjects]\n",
    "\n",
    "    preprocessed = pd.DataFrame(columns=columns)\n",
    "    for identifier, word in identifiers.items():\n",
    "        measure_by_subj = [measures[identifier][subject] for subject in ordered_subjects]\n",
    "        new_row = [identifier, word] + measure_by_subj\n",
    "        n = len(preprocessed)\n",
    "        preprocessed.loc[n] = new_row\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "def save_preprocessed(corpus_name, varname, preproc_data):\n",
    "    \"\"\"Save dataframe as a .tsv file.\"\"\"\n",
    "    preproc_data.to_csv(PATH_TO_HUMAN_DATA / f'{corpus_name}_{varname.name}.tsv', sep='\\t')  \n",
    "\n",
    "\n",
    "def save_meta(corpus_name, texts, text_ids, subject_ids):\n",
    "    \"\"\"Save the text id, subject id and texts as JSON files.\"\"\"\n",
    "    with open(PATH_TO_TEXTS / f'{corpus_name}.json', 'w') as file:\n",
    "        json.dump(texts, file)\n",
    "\n",
    "    with open(PATH_TO_MAPS / f'{corpus_name}_subjects.json', 'w') as file:\n",
    "        json.dump(subject_ids, file)\n",
    "\n",
    "    with open(PATH_TO_MAPS / f'{corpus_name}_texts.json', 'w') as file:\n",
    "        json.dump(text_ids, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RastrOS\n",
    "\n",
    "Preprocessing the Brazilian Portuguese RastrOS corpus. The details have been published in a master thesis [Vieira, 2020](https://repositorio.ufc.br/handle/riufc/55798) and also in [Leal et al. (2022)](https://doi.org/10.1007/s10579-022-09609-0).\n",
    "\n",
    "- Eye Link 1000 Hz (SR Research)\n",
    "- 37 participants, all read all paragraphs\n",
    "- 50 paragraphs\n",
    "- 120 sentences\n",
    "- 2494 words; 2831 tokens (with punctuation)\n",
    "- journalistic, literary, popular science\n",
    "- Participants read paragraphs, one by one in a random order. \n",
    "\n",
    "We follow the documentation described in Table 5 in Leal et al. (2022). For each subject, we retrieve the ```Word_Unique_ID``` which here conveniently encodes the text/paragraph ID and the word position in the text. We also get the word as it was shown in the screen from ```IA_LABEL``` (or ```Word```, see discussion below) and the measures. We use the ```IA_REGRESSION_OUT``` measure in the file ```RastrOS_Corpus_Eytracking_Data.tsv```, which contains:\n",
    "\n",
    "> Whether regression(s) was made from the current interest area to earlier interest areas (e.g., previous parts of the sentence) prior to leaving that interest area in a forward direction. 1 if a saccade exits the current interest area to a lower IA_ID (to the left in English) before a later interest area was fixated; 0 if not.\n",
    "\n",
    "We need the help of the variable ```IA_SKIP```:\n",
    "\n",
    "> An interest area is considered skipped (i.e.,IA_SKIP = 1) if no fixation occurred in first-pass reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RASTROS_NAME = 'rastros_ptbr'\n",
    "PATH_TO_RASTROS = Path('data/RastrOS/osfstorage/')\n",
    "\n",
    "rastros_measures = {\n",
    "    'IA_REGRESSION_OUT': FPREGOUT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the quoting none because of one field in which the word begins with \"\n",
    "rastros_raw = pd.read_csv(PATH_TO_RASTROS / 'RastrOS_Corpus_Eyetracking_Data.tsv', \n",
    "                          sep='\\t', low_memory=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECORDING_SESSION_LABEL</th>\n",
       "      <th>Word_Unique_ID</th>\n",
       "      <th>Text_ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Word_Number</th>\n",
       "      <th>Sentence_Number</th>\n",
       "      <th>Word_In_Sentence_Number</th>\n",
       "      <th>Word_Place_In_Sent</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word_Cleaned</th>\n",
       "      <th>...</th>\n",
       "      <th>IA_REGRESSION_IN_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT</th>\n",
       "      <th>IA_REGRESSION_OUT_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT_FULL</th>\n",
       "      <th>IA_REGRESSION_OUT_FULL_COUNT</th>\n",
       "      <th>IA_REGRESSION_PATH_DURATION</th>\n",
       "      <th>IA_FIRST_SACCADE_AMPLITUDE</th>\n",
       "      <th>IA_FIRST_SACCADE_ANGLE</th>\n",
       "      <th>IA_FIRST_SACCADE_START_TIME</th>\n",
       "      <th>IA_FIRST_SACCADE_END_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C01</td>\n",
       "      <td>UID_13_1</td>\n",
       "      <td>13</td>\n",
       "      <td>DC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mudanças</td>\n",
       "      <td>mudanças</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C01</td>\n",
       "      <td>UID_13_2</td>\n",
       "      <td>13</td>\n",
       "      <td>DC</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>climáticas</td>\n",
       "      <td>climáticas</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>306</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.20</td>\n",
       "      <td>224</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C01</td>\n",
       "      <td>UID_13_3</td>\n",
       "      <td>13</td>\n",
       "      <td>DC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>estão</td>\n",
       "      <td>estão</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-177.52</td>\n",
       "      <td>850</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C01</td>\n",
       "      <td>UID_13_4</td>\n",
       "      <td>13</td>\n",
       "      <td>DC</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>aquecendo</td>\n",
       "      <td>aquecendo</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "      <td>5.84</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>613</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C01</td>\n",
       "      <td>UID_13_5</td>\n",
       "      <td>13</td>\n",
       "      <td>DC</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87878</th>\n",
       "      <td>I5</td>\n",
       "      <td>UID_39_45</td>\n",
       "      <td>39</td>\n",
       "      <td>JN</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>quilômetros</td>\n",
       "      <td>quilômetros</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>308</td>\n",
       "      <td>4.09</td>\n",
       "      <td>3.93</td>\n",
       "      <td>11194</td>\n",
       "      <td>11229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87879</th>\n",
       "      <td>I5</td>\n",
       "      <td>UID_39_46</td>\n",
       "      <td>39</td>\n",
       "      <td>JN</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87880</th>\n",
       "      <td>I5</td>\n",
       "      <td>UID_39_47</td>\n",
       "      <td>39</td>\n",
       "      <td>JN</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>vários</td>\n",
       "      <td>vários</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.99</td>\n",
       "      <td>11538</td>\n",
       "      <td>11580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87881</th>\n",
       "      <td>I5</td>\n",
       "      <td>UID_39_48</td>\n",
       "      <td>39</td>\n",
       "      <td>JN</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>trechos</td>\n",
       "      <td>trechos</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>3.87</td>\n",
       "      <td>1.41</td>\n",
       "      <td>11874</td>\n",
       "      <td>11913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87882</th>\n",
       "      <td>I5</td>\n",
       "      <td>UID_39_49</td>\n",
       "      <td>39</td>\n",
       "      <td>JN</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>submersos.</td>\n",
       "      <td>submersos</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>917</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12255</td>\n",
       "      <td>12291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87883 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RECORDING_SESSION_LABEL Word_Unique_ID  Text_ID Genre  Word_Number   \n",
       "0                         C01       UID_13_1       13    DC            1  \\\n",
       "1                         C01       UID_13_2       13    DC            2   \n",
       "2                         C01       UID_13_3       13    DC            3   \n",
       "3                         C01       UID_13_4       13    DC            4   \n",
       "4                         C01       UID_13_5       13    DC            5   \n",
       "...                       ...            ...      ...   ...          ...   \n",
       "87878                      I5      UID_39_45       39    JN           45   \n",
       "87879                      I5      UID_39_46       39    JN           46   \n",
       "87880                      I5      UID_39_47       39    JN           47   \n",
       "87881                      I5      UID_39_48       39    JN           48   \n",
       "87882                      I5      UID_39_49       39    JN           49   \n",
       "\n",
       "       Sentence_Number  Word_In_Sentence_Number  Word_Place_In_Sent   \n",
       "0                    1                        1                   1  \\\n",
       "1                    1                        2                   1   \n",
       "2                    1                        3                   1   \n",
       "3                    1                        4                   1   \n",
       "4                    1                        5                   2   \n",
       "...                ...                      ...                 ...   \n",
       "87878                3                        7                   3   \n",
       "87879                3                        8                   3   \n",
       "87880                3                        9                   4   \n",
       "87881                3                       10                   4   \n",
       "87882                3                       11                   4   \n",
       "\n",
       "              Word Word_Cleaned  ...  IA_REGRESSION_IN_COUNT   \n",
       "0         Mudanças     mudanças  ...                       1  \\\n",
       "1       climáticas   climáticas  ...                       0   \n",
       "2            estão        estão  ...                       1   \n",
       "3        aquecendo    aquecendo  ...                       0   \n",
       "4                o            o  ...                       .   \n",
       "...            ...          ...  ...                     ...   \n",
       "87878  quilômetros  quilômetros  ...                       0   \n",
       "87879            e            e  ...                       .   \n",
       "87880       vários       vários  ...                       0   \n",
       "87881      trechos      trechos  ...                       0   \n",
       "87882   submersos.    submersos  ...                       0   \n",
       "\n",
       "       IA_REGRESSION_OUT  IA_REGRESSION_OUT_COUNT  IA_REGRESSION_OUT_FULL   \n",
       "0                      0                        0                       0  \\\n",
       "1                      0                        0                       0   \n",
       "2                      0                        0                       0   \n",
       "3                      1                        1                       1   \n",
       "4                      .                        .                       .   \n",
       "...                  ...                      ...                     ...   \n",
       "87878                  0                        0                       0   \n",
       "87879                  .                        .                       .   \n",
       "87880                  0                        0                       0   \n",
       "87881                  0                        0                       0   \n",
       "87882                  0                        0                       0   \n",
       "\n",
       "       IA_REGRESSION_OUT_FULL_COUNT IA_REGRESSION_PATH_DURATION   \n",
       "0                                 0                         157  \\\n",
       "1                                 0                         306   \n",
       "2                                 0                         122   \n",
       "3                                 1                         319   \n",
       "4                                 .                           .   \n",
       "...                             ...                         ...   \n",
       "87878                             0                         308   \n",
       "87879                             .                           .   \n",
       "87880                             0                         293   \n",
       "87881                             0                         341   \n",
       "87882                             0                         917   \n",
       "\n",
       "       IA_FIRST_SACCADE_AMPLITUDE  IA_FIRST_SACCADE_ANGLE   \n",
       "0                               .                       .  \\\n",
       "1                            4.14                    2.20   \n",
       "2                            1.41                 -177.52   \n",
       "3                            5.84                   -0.22   \n",
       "4                               .                       .   \n",
       "...                           ...                     ...   \n",
       "87878                        4.09                    3.93   \n",
       "87879                           .                       .   \n",
       "87880                        5.23                    4.99   \n",
       "87881                        3.87                    1.41   \n",
       "87882                        4.60                    0.19   \n",
       "\n",
       "      IA_FIRST_SACCADE_START_TIME IA_FIRST_SACCADE_END_TIME  \n",
       "0                               .                         .  \n",
       "1                             224                       270  \n",
       "2                             850                       878  \n",
       "3                             613                       652  \n",
       "4                               .                         .  \n",
       "...                           ...                       ...  \n",
       "87878                       11194                     11229  \n",
       "87879                           .                         .  \n",
       "87880                       11538                     11580  \n",
       "87881                       11874                     11913  \n",
       "87882                       12255                     12291  \n",
       "\n",
       "[87883 rows x 72 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rastros_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the values used for each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first-pass-regression-out\n",
      "Counter({'0': 58254, '.': 17715, '1': 11914})\n",
      "NaNs:  0\n",
      "\n",
      "first pass skips\n",
      "Counter({0: 59061, 1: 28822})\n",
      "NaNs:  0\n"
     ]
    }
   ],
   "source": [
    "for column, measure_type in rastros_measures.items():\n",
    "    print(f'\\n{measure_type.name}')\n",
    "    print(Counter(rastros_raw[column]))\n",
    "    print('NaNs: ', rastros_raw[column].isna().sum())\n",
    "\n",
    "print('\\nfirst pass skips')\n",
    "print(Counter(rastros_raw['IA_SKIP']))\n",
    "print('NaNs: ', rastros_raw['IA_SKIP'].isna().sum())\n",
    "\n",
    "\n",
    "assert set(rastros_raw['IA_REGRESSION_OUT'].unique()) == set(['0', '1', '.'])\n",
    "assert set(rastros_raw['IA_SKIP'].unique()) == set([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column we are mostly interested in (```IA_REGRESSION_OUT```) sometimes contains a ```.```. Let's investigate why. But none of the measures has ```NaN```s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in rastros_raw.iterrows():\n",
    "    regression_label = row['IA_REGRESSION_OUT']\n",
    "    assert regression_label in ('0', '1', '.')\n",
    "    if regression_label in ('0', '1'):\n",
    "        if row['IA_SKIP'] == 1.:\n",
    "            # tokens skipped at first pass but fixated later \n",
    "            assert row['IA_FIXATION_COUNT'] > 0\n",
    "            # they use regression 0 for these cases\n",
    "            assert regression_label == '0'\n",
    "        # if there is a label, the token was fixated at least once\n",
    "        assert row['IA_DWELL_TIME'] > 0.\n",
    "    if regression_label == '.':\n",
    "        # the dots imply skipped at first pass and also skipped altogether\n",
    "        assert row['IA_SKIP'] == 1.\n",
    "        assert row['IA_DWELL_TIME'] == 0.\n",
    "        assert row['IA_FIXATION_COUNT'] == 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dots correspond to skips, but not all skips correspond to dots. When we inspect the dwell time, we see that dots occur only in tokens whose total dwell time is $0$. So if a word was never fixated, ```IA_REGRESSION_OUT``` contains a dot. If it was fixated, it contains either $0$ or $1$, but it may also be $0$ when it has been skipped at the first pass (which is what ```IA_SKIP``` means.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mappings from subject ids and texts ids to integers, which will be used to create our identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rastros_subject_ids = {subj_id: i for i, subj_id in enumerate(rastros_raw['RECORDING_SESSION_LABEL'].unique())}\n",
    "rastros_text_ids = {int(text_id): i for i, text_id in enumerate(rastros_raw['Text_ID'].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a list of token ids and their corresponding token, making sure that is is consistent across subjects. We also check whether the ```Word``` column contains the same token as the ```IA_LABEL```, which is the string actually shown at the screen. \n",
    "\n",
    "For some reason, the row UID_8_9 has a wrong label for the text id. So we'll split the Word_Unique_ID to retrieve the text and token instead of using the columns Text_ID and Word_Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastros_clean(word):\n",
    "    \"\"\"Manually clean up some encoding issues that could not be solved.\"\"\"\n",
    "    cleaned = word.replace('\\x96', '--').replace('\\x94', '”').replace('\\x93', '“').replace('\\x97', '--').replace('\\xa0', '')\n",
    "    return cleaned\n",
    "\n",
    "rastros_identifiers = {}\n",
    "inconsistent_words = {}\n",
    "\n",
    "for index, row in rastros_raw.iterrows():\n",
    "    _, orig_text_id, orig_token_number = row['Word_Unique_ID'].split('_')\n",
    "    text_id = rastros_text_ids[int(orig_text_id)]\n",
    "    token_position = int(orig_token_number) - 1\n",
    "    identifier = TOKEN_ID.format(text_id, token_position)\n",
    "\n",
    "    word = row['Word']\n",
    "    # Deciding which word column to use\n",
    "    if row['Word'] != row['IA_LABEL']:\n",
    "        if row['Word'] == row['IA_LABEL'].replace('.', ','):\n",
    "            # cases where commas have been replaced by full stops in the IA_LABEL\n",
    "            # the author said it was a postprocessing mistake\n",
    "            # in such cases, we stick to row['Word'] containing the comma\n",
    "            pass  \n",
    "        elif row['Word'] == row['IA_LABEL'].replace('\\x92', \"’\").replace('\\x94', '”').replace('\\x93', '“').replace('.', ','):\n",
    "            # these cases seem to be an encoding issue in the file that we\n",
    "            # could not solve using pandas\n",
    "            # in these cases, we'll also use row['Word'] containing the correct \n",
    "            # quotes (assuming that the humans saw the correct text)\n",
    "            pass\n",
    "        else:\n",
    "            # for other inconsistent cases (35 when we counted), we'll stick to IA_LABEL instead\n",
    "            # with some manual cleaning of encoding issues\n",
    "            word = rastros_clean(row['IA_LABEL'])\n",
    "            inconsistent_words[index] = (row['Word'], row['IA_LABEL'], word)\n",
    "    \n",
    "    if identifier not in rastros_identifiers:\n",
    "        rastros_identifiers[identifier] = word\n",
    "    # check that identifiers accross different subjects always map to the same word\n",
    "    assert rastros_identifiers[identifier] == word\n",
    "\n",
    "assert len(rastros_identifiers) == 2494  # total number of tokens from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows have mismatches between 'Word' and 'IA_LABEL', mostly because IA_LABEL seems to replace commas by full stops or due to encoding issues that I could not fix while reading the csv. We have sent an email to the authors to know what exactly did the subjects see with respect to the commas. They said that they may have been replaced by full stops in a post processing step.\n",
    "\n",
    "So our approach is: if 'Word' and 'IA_LABEL' differ only because of swapped commas or encoding issues, we stick to 'Word'. Otherwise, we use 'IA_LABEL', manually cleaning/replacing the \\x symbols. This is only necessary for 33 instances, so probably not a big deal, but should be mentioned in the report. The authors said that IA_LABEL is what was shown in the eye-tracking experiment, and Word was used for the cloze task experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 tokens with remaining mismatches!\n"
     ]
    }
   ],
   "source": [
    "n_mismatches = len(set(inconsistent_words.values()))\n",
    "print(f'There are {n_mismatches} tokens with remaining mismatches!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These subjects have missing data and will be replaced by NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C01\n",
      "C03\n",
      "C04\n",
      "C06\n",
      "C15\n",
      "E02\n",
      "E05\n",
      "E07\n",
      "E12\n",
      "I06\n",
      "I09\n",
      "I16\n",
      "I17\n",
      "I19\n",
      "I21\n"
     ]
    }
   ],
   "source": [
    "for s_id in rastros_subject_ids:\n",
    "    n_obs = rastros_raw[rastros_raw['RECORDING_SESSION_LABEL'] == s_id].shape[0]\n",
    "    if n_obs != len(rastros_identifiers):\n",
    "        print(s_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the texts. Although, in this corpus, the Word_Unique_ID already conveniently encodes the text id and the word position in the text, we reconstruct the texts from our internal integers instead, for consistency with the other corpora. We check that the resulting texts contain all indexes from 0 to the text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rastros_texts = {i: {} for i in rastros_text_ids.values()}\n",
    "\n",
    "for identifier, word in rastros_identifiers.items():\n",
    "    _, text_id, _, word_position = identifier.split('_')\n",
    "    assert int(word_position) not in rastros_texts[int(text_id)]\n",
    "    rastros_texts[int(text_id)][int(word_position)] = word\n",
    "\n",
    "for token_ids in rastros_texts.values():\n",
    "    assert set(token_ids.keys()) == set(range(len(token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix an issue with the open quotation mark in the first position\n",
    "# otherwise it leads to an error in the adjusted outputs for the models\n",
    "rastros_texts[27][0] = rastros_texts[27][0].replace('\"', '“')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the measure we want. We initialise the measures dictionary with ```MISSING_LABEL``` for all identifiers and for all subjects, so any missing data (subjects that do not contain data for all texts) will be the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rastros_measures(measure, subjects, identifiers, data, replacer):\n",
    "    \"\"\"Create dictionary with measure for each subject in RastrOS data.\"\"\"\n",
    "    # initialize the dictionary with empty values, so that all identifiers have\n",
    "    # all subject keys, even when no data was collected for them\n",
    "    measures = {identifier: {subject: MISSING_LABEL for subject in subjects.values()} for identifier in identifiers.keys()}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        _, orig_text_id, orig_token_number = row['Word_Unique_ID'].split('_')\n",
    "        text_id = rastros_text_ids[int(orig_text_id)]\n",
    "        token_position = int(orig_token_number) - 1\n",
    "        identifier = TOKEN_ID.format(text_id, token_position)\n",
    "        # sanity check that the word or ia column matches\n",
    "        assert (identifiers[identifier] == row['Word'] \n",
    "                or identifiers[identifier] == row['IA_LABEL'] \n",
    "                or index in inconsistent_words)\n",
    "\n",
    "        orig_subject = row['RECORDING_SESSION_LABEL']\n",
    "        subject = subjects[orig_subject]\n",
    "        \n",
    "        measure_value = row[measure]\n",
    "        was_skipped = row['IA_SKIP']\n",
    "        # no NaNs in this corpus\n",
    "        assert not pd.isna(measure_value)\n",
    "        assert was_skipped in (0., 1.)\n",
    "\n",
    "        # skipped words get replaced with chosen label\n",
    "        if was_skipped == 1. and replacer is not None:\n",
    "            measure_value = replacer\n",
    "        else:\n",
    "            measure_value = float(measure_value)\n",
    "\n",
    "        # each observation should be unique, otherwise there is a problem in the data\n",
    "        # we check that the value is empty before adding it\n",
    "        assert np.isnan(measures[identifier][subject])\n",
    "        measures[identifier][subject] = measure_value\n",
    "\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save dataframes with the format described at the top of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta(RASTROS_NAME, rastros_texts, rastros_text_ids, rastros_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix order of the subjects across dataframes\n",
    "ordered_subjs = list(rastros_subject_ids.values())\n",
    "\n",
    "def rastros_build_and_save(column_name, variable_name, replacer):\n",
    "    measures = get_rastros_measures(column_name,\n",
    "                                    rastros_subject_ids,\n",
    "                                    rastros_identifiers,\n",
    "                                    rastros_raw,\n",
    "                                    replacer)\n",
    "\n",
    "    preproc = create_dataframe(variable_name,\n",
    "                               measures,\n",
    "                               rastros_identifiers,\n",
    "                               ordered_subjs)\n",
    "    save_preprocessed(RASTROS_NAME, variable_name, preproc)\n",
    "    return preproc\n",
    "\n",
    "\n",
    "# first pass regressions, binary -- replace '.' with SKIP_LABEL\n",
    "rastros_fpregs = rastros_build_and_save('IA_REGRESSION_OUT', FPREGOUT, replacer=SKIP_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier             0\n",
       "Token                  0\n",
       "fpregout:Subj_0      100\n",
       "fpregout:Subj_1        0\n",
       "fpregout:Subj_2       51\n",
       "fpregout:Subj_3       56\n",
       "fpregout:Subj_4        0\n",
       "fpregout:Subj_5       40\n",
       "fpregout:Subj_6        0\n",
       "fpregout:Subj_7        0\n",
       "fpregout:Subj_8        0\n",
       "fpregout:Subj_9        0\n",
       "fpregout:Subj_10     270\n",
       "fpregout:Subj_11       0\n",
       "fpregout:Subj_12      39\n",
       "fpregout:Subj_13       0\n",
       "fpregout:Subj_14     588\n",
       "fpregout:Subj_15      55\n",
       "fpregout:Subj_16       0\n",
       "fpregout:Subj_17       0\n",
       "fpregout:Subj_18       0\n",
       "fpregout:Subj_19     296\n",
       "fpregout:Subj_20       0\n",
       "fpregout:Subj_21       0\n",
       "fpregout:Subj_22       0\n",
       "fpregout:Subj_23      36\n",
       "fpregout:Subj_24       0\n",
       "fpregout:Subj_25       0\n",
       "fpregout:Subj_26      49\n",
       "fpregout:Subj_27       0\n",
       "fpregout:Subj_28       0\n",
       "fpregout:Subj_29       0\n",
       "fpregout:Subj_30     440\n",
       "fpregout:Subj_31      96\n",
       "fpregout:Subj_32       0\n",
       "fpregout:Subj_33     407\n",
       "fpregout:Subj_34       0\n",
       "fpregout:Subj_35    1872\n",
       "fpregout:Subj_36       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rastros_fpregs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Token</th>\n",
       "      <th>fpregout:Subj_0</th>\n",
       "      <th>fpregout:Subj_1</th>\n",
       "      <th>fpregout:Subj_2</th>\n",
       "      <th>fpregout:Subj_3</th>\n",
       "      <th>fpregout:Subj_4</th>\n",
       "      <th>fpregout:Subj_5</th>\n",
       "      <th>fpregout:Subj_6</th>\n",
       "      <th>fpregout:Subj_7</th>\n",
       "      <th>...</th>\n",
       "      <th>fpregout:Subj_27</th>\n",
       "      <th>fpregout:Subj_28</th>\n",
       "      <th>fpregout:Subj_29</th>\n",
       "      <th>fpregout:Subj_30</th>\n",
       "      <th>fpregout:Subj_31</th>\n",
       "      <th>fpregout:Subj_32</th>\n",
       "      <th>fpregout:Subj_33</th>\n",
       "      <th>fpregout:Subj_34</th>\n",
       "      <th>fpregout:Subj_35</th>\n",
       "      <th>fpregout:Subj_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_0_token_0</td>\n",
       "      <td>Mudanças</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_0_token_1</td>\n",
       "      <td>climáticas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_0_token_2</td>\n",
       "      <td>estão</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_0_token_3</td>\n",
       "      <td>aquecendo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_0_token_4</td>\n",
       "      <td>o</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>text_43_token_43</td>\n",
       "      <td>Tais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>text_43_token_44</td>\n",
       "      <td>objetos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>text_43_token_45</td>\n",
       "      <td>são</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>text_43_token_46</td>\n",
       "      <td>chamados</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>text_43_token_47</td>\n",
       "      <td>traçadores.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2494 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Identifier        Token  fpregout:Subj_0  fpregout:Subj_1   \n",
       "0       text_0_token_0     Mudanças              0.0              0.0  \\\n",
       "1       text_0_token_1   climáticas              0.0              0.0   \n",
       "2       text_0_token_2        estão             -1.0              0.0   \n",
       "3       text_0_token_3    aquecendo              1.0              0.0   \n",
       "4       text_0_token_4            o             -1.0             -1.0   \n",
       "...                ...          ...              ...              ...   \n",
       "2489  text_43_token_43         Tais              NaN              0.0   \n",
       "2490  text_43_token_44      objetos              NaN              0.0   \n",
       "2491  text_43_token_45          são              NaN             -1.0   \n",
       "2492  text_43_token_46     chamados              NaN              0.0   \n",
       "2493  text_43_token_47  traçadores.              NaN              0.0   \n",
       "\n",
       "      fpregout:Subj_2  fpregout:Subj_3  fpregout:Subj_4  fpregout:Subj_5   \n",
       "0                 0.0              0.0              0.0              0.0  \\\n",
       "1                 0.0              0.0              0.0              0.0   \n",
       "2                 0.0              0.0              0.0              0.0   \n",
       "3                 0.0              1.0              0.0              0.0   \n",
       "4                -1.0              1.0             -1.0             -1.0   \n",
       "...               ...              ...              ...              ...   \n",
       "2489              0.0              0.0              0.0              0.0   \n",
       "2490              0.0              0.0              0.0              0.0   \n",
       "2491             -1.0             -1.0             -1.0             -1.0   \n",
       "2492              0.0              1.0              0.0              0.0   \n",
       "2493              1.0              0.0              1.0              1.0   \n",
       "\n",
       "      fpregout:Subj_6  fpregout:Subj_7  ...  fpregout:Subj_27   \n",
       "0                 0.0              0.0  ...               0.0  \\\n",
       "1                -1.0              1.0  ...               0.0   \n",
       "2                -1.0              0.0  ...              -1.0   \n",
       "3                -1.0              0.0  ...               1.0   \n",
       "4                -1.0             -1.0  ...              -1.0   \n",
       "...               ...              ...  ...               ...   \n",
       "2489              0.0              0.0  ...               0.0   \n",
       "2490              0.0              0.0  ...               0.0   \n",
       "2491              0.0             -1.0  ...              -1.0   \n",
       "2492              0.0              0.0  ...               0.0   \n",
       "2493              0.0              1.0  ...               1.0   \n",
       "\n",
       "      fpregout:Subj_28  fpregout:Subj_29  fpregout:Subj_30  fpregout:Subj_31   \n",
       "0                  0.0               0.0               NaN               0.0  \\\n",
       "1                  0.0               0.0               NaN               0.0   \n",
       "2                  0.0               1.0               NaN               0.0   \n",
       "3                  0.0               0.0               NaN               0.0   \n",
       "4                 -1.0               0.0               NaN              -1.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2489               1.0               0.0              -1.0              -1.0   \n",
       "2490               0.0               0.0              -1.0              -1.0   \n",
       "2491              -1.0              -1.0              -1.0               0.0   \n",
       "2492               0.0               1.0               0.0               0.0   \n",
       "2493               0.0               1.0               1.0               1.0   \n",
       "\n",
       "      fpregout:Subj_32  fpregout:Subj_33  fpregout:Subj_34  fpregout:Subj_35   \n",
       "0                  0.0               0.0               0.0               NaN  \\\n",
       "1                  0.0               1.0               1.0               NaN   \n",
       "2                  0.0               0.0               0.0               NaN   \n",
       "3                  1.0               0.0               0.0               NaN   \n",
       "4                 -1.0               1.0              -1.0               NaN   \n",
       "...                ...               ...               ...               ...   \n",
       "2489               0.0               NaN               0.0               NaN   \n",
       "2490               0.0               NaN               0.0               NaN   \n",
       "2491               0.0               NaN               0.0               NaN   \n",
       "2492               0.0               NaN              -1.0               NaN   \n",
       "2493               1.0               NaN               1.0               NaN   \n",
       "\n",
       "      fpregout:Subj_36  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                 -1.0  \n",
       "...                ...  \n",
       "2489               0.0  \n",
       "2490               0.0  \n",
       "2491              -1.0  \n",
       "2492               0.0  \n",
       "2493               0.0  \n",
       "\n",
       "[2494 rows x 39 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rastros_fpregs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoTeC\n",
    "Preprocessing the German Potsdam Textbook Corpus, available at [OSF](https://osf.io/dn5hp/).\n",
    "\n",
    "- SR Research Eyelink 1000 eyetracker\n",
    "- 75 (or 62 valid?) participants, all read all texts\n",
    "- 12 scientific texts (6 biology and 6 physics)\n",
    "- On average 158 words per text\n",
    "- Each text fits onto a single screen\n",
    "- Apparently is contains no punctuation\n",
    "\n",
    "We follow the documentation described in their OSF wiki and use the ```FPReg```\n",
    "measure in the ```eyetracking_data/readingMeasures/*.txt```, which contains:\n",
    "\n",
    "> 1 if a regression was initiated in the first-pass reading of the word, otherwise 0 (sign(RPD exc))\n",
    "\n",
    "We need the auxiliary ```FPF``` to identify skipped tokens: \n",
    "\n",
    "> 1 if the word was fixated in the first-pass, otherwise 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first extract the words from ```eyetracking_data/texts/texts_tags/<TextID>.tags```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "POTEC_NAME = 'potec_de'\n",
    "PATH_TO_POTEC = Path('data/PoTeC/osfstorage/')\n",
    "\n",
    "potec_measures = {'FPReg': FPREGOUT}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract texts from the raw text files. It's hard to extract the punctuation precisely because no documentation is available. So we'll add only commas and full stops.\n",
    "\n",
    "Jäger's paper mentions that 13 subjects were removed in Makovski's paper due to poor calibration. Based on their ```mergeFixationsWordfeatures.py``` script, the subject IDs to be removed are:\n",
    "\n",
    "-  2, 9, 16, 19, 22, 31, 39, 41, 64, 72, 83, 85, 90, 93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "potec_text_ids = {}\n",
    "potec_texts = {}\n",
    "potec_subject_ids = {}\n",
    "\n",
    "def add_punctuation(before, word, after):\n",
    "    text = word\n",
    "    # according to STTS documentation, $( means \"other punctuation\"\n",
    "    # some other columns mention parenthesis, quotes, but no documentation\n",
    "    # since we cannot know for sure, we'll use nothing\n",
    "    # https://www.cis.lmu.de/~schmid/tools/TreeTagger/data/STTS-Tagset.pdf\n",
    "    # the other \"$.\" if final sentence, although we also don't know exactly\n",
    "    # which, we will use .\n",
    "    assert before in ('$(', 'None')\n",
    "    # ignore pure '$(' in before because we don't know what is was\n",
    "    if after not in ('$(', 'None'):\n",
    "        after = after.replace('$(', '')\n",
    "        assert after in ('$.', '$,')\n",
    "        text = text + after.replace('$', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "# from https://stackoverflow.com/a/56469905\n",
    "with os.scandir(PATH_TO_POTEC / 'texts' / 'texts_tags') as directory:\n",
    "    for entry in directory:\n",
    "        if entry.name.endswith('.tags') and entry.is_file():\n",
    "            text_id, _ = entry.name.split('.')\n",
    "            text_index = len(potec_text_ids)\n",
    "            potec_text_ids[text_id] = text_index\n",
    "            potec_texts[text_index] = {} \n",
    "            with open(entry.path, 'r') as file:\n",
    "                for word_id, line in enumerate(file.readlines()[1:]):\n",
    "                    _, word, _, _, _, _, _, _, punc_before, punc_after, *_ = line.split('\\t')\n",
    "                    ia_label = add_punctuation(punc_before, word, punc_after)\n",
    "                    potec_texts[text_index][word_id] = ia_label\n",
    "\n",
    "\n",
    "POTEC_EXCLUDED_SUBJS = [2, 9, 16, 19, 22, 31, 39, 41, 64, 72, 83, 85, 90, 93]\n",
    "with os.scandir(PATH_TO_POTEC / 'eyetracking_data' / 'eyetracking_data' / 'readingMeasures') as directory:\n",
    "    for entry in directory:\n",
    "        if entry.name.endswith(\".txt\") and entry.is_file():\n",
    "            subj_id, *_ = entry.name.strip('.txt').split('_')\n",
    "            # add new subject\n",
    "            if subj_id not in potec_subject_ids:\n",
    "                if int(subj_id.replace('reader', '')) not in POTEC_EXCLUDED_SUBJS:\n",
    "                    potec_subject_ids[subj_id] = len(potec_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "potec_identifiers = {TOKEN_ID.format(text_id, word_position): word \n",
    "                     for text_id, words in potec_texts.items() for word_position, word in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix two issues with the full stops\n",
    "# probably this should not be a full stop because it's in the middle of a sentence, so we'll use the other possibility which is ;\n",
    "potec_texts[5][46] = potec_texts[5][46].replace('Stärke.', 'Stärke;')\n",
    "potec_texts[6][84] = potec_texts[6][84].replace('sind.', 'sind;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the measures from each ```reader<ID>_<TextID>_rm.txt``` file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "potec_data_files = []\n",
    "# from https://stackoverflow.com/a/56469905\n",
    "with os.scandir(PATH_TO_POTEC / 'eyetracking_data' / 'eyetracking_data' / 'readingMeasures') as directory:\n",
    "    for entry in directory:\n",
    "        if entry.name.endswith(\".txt\") and entry.is_file():\n",
    "            potec_data_files.append(entry)\n",
    "\n",
    "def get_potec_measures(measure, subjects, identifiers):\n",
    "    measures = {identifier: {subj: MISSING_LABEL for subj in potec_subject_ids.values()} for identifier in potec_identifiers.keys()}\n",
    "    for entry in potec_data_files:\n",
    "        orig_subj_id, orig_text_id, _ = entry.name.strip('.txt').split('_')\n",
    "        if int(orig_subj_id.replace('reader', '')) in POTEC_EXCLUDED_SUBJS:\n",
    "            # ignore the files of reader with poor calibration (according to the original code)\n",
    "            continue\n",
    "        subj_id = potec_subject_ids[orig_subj_id]\n",
    "        text_id = potec_text_ids[orig_text_id]\n",
    "        data = pd.read_csv(entry.path, sep='\\t')\n",
    "        assert data.shape[0] == len(potec_texts[text_id])\n",
    "        for index, row in data.iterrows():\n",
    "            measure_value = float(row[measure])\n",
    "            # no missing values in the corpus\n",
    "            assert not pd.isna(measure_value)\n",
    "            assert measure_value in (0., 1.)\n",
    "            # skip is the opposite of first pass fix\n",
    "            assert float(row['FPF']) in (0., 1.)\n",
    "            was_skipped = 1. - float(row['FPF'])\n",
    "            if was_skipped == 1.:\n",
    "                assert measure_value == 0.\n",
    "                measure_value = SKIP_LABEL\n",
    "            identifier = TOKEN_ID.format(text_id, index)\n",
    "            # value should be empty until we add it\n",
    "            assert np.isnan(measures[identifier][subj_id])\n",
    "            measures[identifier][subj_id] = measure_value\n",
    "    \n",
    "    for subj_data in measures.values():\n",
    "        assert sum([np.isnan(value) for value in subj_data.values()]) == 0\n",
    "\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta(POTEC_NAME, potec_texts, potec_text_ids, potec_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "potec_ordered_subjs = list(potec_subject_ids.values())\n",
    "\n",
    "def potec_build_and_save(column_name, variable_name):\n",
    "    measures = get_potec_measures(column_name, potec_subject_ids, potec_identifiers)\n",
    "    preproc = create_dataframe(variable_name, measures, potec_identifiers, potec_ordered_subjs)\n",
    "    save_preprocessed(POTEC_NAME, variable_name, preproc)\n",
    "    return preproc\n",
    "\n",
    "potec_fpregs = potec_build_and_save('FPReg', FPREGOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier          0\n",
       "Token               0\n",
       "fpregout:Subj_0     0\n",
       "fpregout:Subj_1     0\n",
       "fpregout:Subj_2     0\n",
       "                   ..\n",
       "fpregout:Subj_57    0\n",
       "fpregout:Subj_58    0\n",
       "fpregout:Subj_59    0\n",
       "fpregout:Subj_60    0\n",
       "fpregout:Subj_61    0\n",
       "Length: 64, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potec_fpregs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Token</th>\n",
       "      <th>fpregout:Subj_0</th>\n",
       "      <th>fpregout:Subj_1</th>\n",
       "      <th>fpregout:Subj_2</th>\n",
       "      <th>fpregout:Subj_3</th>\n",
       "      <th>fpregout:Subj_4</th>\n",
       "      <th>fpregout:Subj_5</th>\n",
       "      <th>fpregout:Subj_6</th>\n",
       "      <th>fpregout:Subj_7</th>\n",
       "      <th>...</th>\n",
       "      <th>fpregout:Subj_52</th>\n",
       "      <th>fpregout:Subj_53</th>\n",
       "      <th>fpregout:Subj_54</th>\n",
       "      <th>fpregout:Subj_55</th>\n",
       "      <th>fpregout:Subj_56</th>\n",
       "      <th>fpregout:Subj_57</th>\n",
       "      <th>fpregout:Subj_58</th>\n",
       "      <th>fpregout:Subj_59</th>\n",
       "      <th>fpregout:Subj_60</th>\n",
       "      <th>fpregout:Subj_61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_0_token_0</td>\n",
       "      <td>Photonische</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_0_token_1</td>\n",
       "      <td>Kristalle</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_0_token_2</td>\n",
       "      <td>sind</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_0_token_3</td>\n",
       "      <td>räumlich</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_0_token_4</td>\n",
       "      <td>periodische</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>text_11_token_136</td>\n",
       "      <td>Wellenlänge</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>text_11_token_137</td>\n",
       "      <td>des</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>text_11_token_138</td>\n",
       "      <td>beleuchtenden</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>text_11_token_139</td>\n",
       "      <td>Lichtes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>text_11_token_140</td>\n",
       "      <td>sind.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1895 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Identifier          Token  fpregout:Subj_0  fpregout:Subj_1   \n",
       "0        text_0_token_0    Photonische              0.0              0.0  \\\n",
       "1        text_0_token_1      Kristalle              0.0              0.0   \n",
       "2        text_0_token_2           sind              0.0              0.0   \n",
       "3        text_0_token_3       räumlich              0.0              0.0   \n",
       "4        text_0_token_4    periodische              0.0              0.0   \n",
       "...                 ...            ...              ...              ...   \n",
       "1890  text_11_token_136    Wellenlänge              0.0              1.0   \n",
       "1891  text_11_token_137            des              0.0             -1.0   \n",
       "1892  text_11_token_138  beleuchtenden              0.0              0.0   \n",
       "1893  text_11_token_139        Lichtes              0.0              0.0   \n",
       "1894  text_11_token_140          sind.              0.0              1.0   \n",
       "\n",
       "      fpregout:Subj_2  fpregout:Subj_3  fpregout:Subj_4  fpregout:Subj_5   \n",
       "0                 0.0              0.0              0.0              0.0  \\\n",
       "1                 0.0              1.0             -1.0              0.0   \n",
       "2                 0.0              0.0              0.0              0.0   \n",
       "3                 0.0              0.0              0.0              0.0   \n",
       "4                 0.0              1.0              0.0              1.0   \n",
       "...               ...              ...              ...              ...   \n",
       "1890              0.0              0.0              0.0              0.0   \n",
       "1891              0.0              1.0             -1.0             -1.0   \n",
       "1892              0.0              1.0              0.0              0.0   \n",
       "1893              0.0              0.0              0.0              0.0   \n",
       "1894             -1.0              1.0              1.0              1.0   \n",
       "\n",
       "      fpregout:Subj_6  fpregout:Subj_7  ...  fpregout:Subj_52   \n",
       "0                 0.0              0.0  ...               0.0  \\\n",
       "1                 0.0              0.0  ...               0.0   \n",
       "2                 0.0              0.0  ...               0.0   \n",
       "3                 0.0              0.0  ...               0.0   \n",
       "4                 0.0              1.0  ...               0.0   \n",
       "...               ...              ...  ...               ...   \n",
       "1890              0.0              1.0  ...               0.0   \n",
       "1891             -1.0             -1.0  ...               0.0   \n",
       "1892              1.0              1.0  ...               0.0   \n",
       "1893              0.0              0.0  ...               0.0   \n",
       "1894              1.0              1.0  ...               1.0   \n",
       "\n",
       "      fpregout:Subj_53  fpregout:Subj_54  fpregout:Subj_55  fpregout:Subj_56   \n",
       "0                  0.0               0.0               0.0               0.0  \\\n",
       "1                  1.0               0.0               1.0               0.0   \n",
       "2                  1.0              -1.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0              -1.0   \n",
       "4                  1.0               1.0               1.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "1890               0.0              -1.0               0.0               0.0   \n",
       "1891               1.0               0.0              -1.0              -1.0   \n",
       "1892               0.0               1.0               1.0               0.0   \n",
       "1893               1.0               0.0               0.0               0.0   \n",
       "1894              -1.0               1.0               1.0               1.0   \n",
       "\n",
       "      fpregout:Subj_57  fpregout:Subj_58  fpregout:Subj_59  fpregout:Subj_60   \n",
       "0                  0.0               0.0               0.0               0.0  \\\n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0              -1.0              -1.0              -1.0   \n",
       "3                  0.0               0.0               1.0               0.0   \n",
       "4                  1.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "1890               0.0              -1.0               1.0               1.0   \n",
       "1891              -1.0              -1.0              -1.0              -1.0   \n",
       "1892               0.0               0.0               1.0               1.0   \n",
       "1893               1.0               0.0               1.0               1.0   \n",
       "1894              -1.0               1.0               1.0               0.0   \n",
       "\n",
       "      fpregout:Subj_61  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  \n",
       "...                ...  \n",
       "1890               0.0  \n",
       "1891               0.0  \n",
       "1892               0.0  \n",
       "1893               0.0  \n",
       "1894               1.0  \n",
       "\n",
       "[1895 rows x 64 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potec_fpregs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that in this corpus there are no missing values\n",
    "for c, column in potec_fpregs[[c for c in potec_fpregs.columns if 'Subj' in c]].items():\n",
    "    assert set(column) == set([0., -1., 1.])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provo\n",
    "Preprocessing the English Provo corpus. \n",
    "\n",
    "- 84 participants\n",
    "- Eye Link 1000 Plus (SR Research)\n",
    "- 55 paragraphs of various sources\n",
    "- The texts were presented in a random order for each participant.\n",
    "\n",
    "We follow the documentation described in Table 2 in Luke and Christianson (2018). We use the ```IA_REGRESSION_OUT``` measure in the file ```Provo_Corpus_Eyetracking_Data.csv```, which contains:\n",
    "\n",
    "> Whether regression(s) was made from the current interest area to earlier interest areas (e.g., previous parts of the sentence) prior to leaving that interest area in a forward direction. 1 if a saccade exits the current interest area to a lower IA_ID (to the left in English) before a later interest area was fixated; 0 if not.\n",
    "\n",
    "We use ```IA_SKIP``` to check first-pass skips:\n",
    "> An interest area is considered skipped (i.e.,IA_SKIP = 1) if no fixation occurred in first-pass reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROVO_NAME = 'provo_en'\n",
    "PATH_TO_PROVO = Path('data/Provo/osfstorage/')\n",
    "\n",
    "provo_measures = {\n",
    "    'IA_REGRESSION_OUT': FPREGOUT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo_texts = {}\n",
    "provo_text_ids = {}\n",
    "\n",
    "PATH_PROVO_TEXTS = PATH_TO_PROVO / 'Provo Corpus Eyelink Program Files and Raw Data' / 'Paragraph Reading' / 'datasets' / 'TRIAL_DataSource_Paragraph_Reading_BLOCKTRIAL.dat'\n",
    "\n",
    "with open(PATH_PROVO_TEXTS, 'r') as file:\n",
    "    for index, line in enumerate(file.readlines()[1:]):\n",
    "        _, text_id, text = line.split('\\t')\n",
    "        provo_text_ids[text_id.strip('\"')] = index\n",
    "        # [1:-1] removes \" at the beginning and the end while keeping cases where there is a double \" due to a real \" in the text\n",
    "        #text = text.strip('\\n')[1:-1].split()\n",
    "        # however, it seems that the IA_LABEL does not contain these quotations in the raw file, \n",
    "        # so we remove them here\n",
    "        text = text.strip('\\n').replace('\"', '').split()\n",
    "        provo_texts[index] = {i: word for i, word in enumerate(text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo_raw = pd.read_csv(PATH_TO_PROVO / 'Provo_Corpus-Eyetracking_Data.csv', sep=',', \n",
    "                        low_memory=False, doublequote=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECORDING_SESSION_LABEL</th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>Word_Unique_ID</th>\n",
       "      <th>Text_ID</th>\n",
       "      <th>Word_Number</th>\n",
       "      <th>Sentence_Number</th>\n",
       "      <th>Word_In_Sentence_Number</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word_Cleaned</th>\n",
       "      <th>Word_Length</th>\n",
       "      <th>...</th>\n",
       "      <th>IA_REGRESSION_IN_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT</th>\n",
       "      <th>IA_REGRESSION_OUT_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT_FULL</th>\n",
       "      <th>IA_REGRESSION_OUT_FULL_COUNT</th>\n",
       "      <th>IA_REGRESSION_PATH_DURATION</th>\n",
       "      <th>IA_FIRST_SACCADE_AMPLITUDE</th>\n",
       "      <th>IA_FIRST_SACCADE_ANGLE</th>\n",
       "      <th>IA_FIRST_SACCADE_END_TIME</th>\n",
       "      <th>IA_FIRST_SACCADE_START_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>Sub01</td>\n",
       "      <td>QID1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>9.31</td>\n",
       "      <td>247.0</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>Sub01</td>\n",
       "      <td>QID2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>now</td>\n",
       "      <td>now</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.89</td>\n",
       "      <td>415.0</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>Sub01</td>\n",
       "      <td>QID3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>rumblings</td>\n",
       "      <td>rumblings</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.46</td>\n",
       "      <td>632.0</td>\n",
       "      <td>609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>Sub01</td>\n",
       "      <td>QID4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>Sub01</td>\n",
       "      <td>QID5</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>apple</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.66</td>\n",
       "      <td>864.0</td>\n",
       "      <td>831.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230407</th>\n",
       "      <td>sub99</td>\n",
       "      <td>Sub84</td>\n",
       "      <td>QID2742</td>\n",
       "      <td>55</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>8728.0</td>\n",
       "      <td>8702.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230408</th>\n",
       "      <td>sub99</td>\n",
       "      <td>Sub84</td>\n",
       "      <td>QID2743</td>\n",
       "      <td>55</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230409</th>\n",
       "      <td>sub99</td>\n",
       "      <td>Sub84</td>\n",
       "      <td>QID2744</td>\n",
       "      <td>55</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>detestable</td>\n",
       "      <td>detestable</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>5.38</td>\n",
       "      <td>8.19</td>\n",
       "      <td>9109.0</td>\n",
       "      <td>9076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230410</th>\n",
       "      <td>sub99</td>\n",
       "      <td>Sub84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>4.01</td>\n",
       "      <td>-3.07</td>\n",
       "      <td>220.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230411</th>\n",
       "      <td>sub99</td>\n",
       "      <td>Sub84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-3.68</td>\n",
       "      <td>9321.0</td>\n",
       "      <td>9291.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230412 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RECORDING_SESSION_LABEL Participant_ID Word_Unique_ID  Text_ID   \n",
       "0                           80          Sub01           QID1        1  \\\n",
       "1                           80          Sub01           QID2        1   \n",
       "2                           80          Sub01           QID3        1   \n",
       "3                           80          Sub01           QID4        1   \n",
       "4                           80          Sub01           QID5        1   \n",
       "...                        ...            ...            ...      ...   \n",
       "230407                   sub99          Sub84        QID2742       55   \n",
       "230408                   sub99          Sub84        QID2743       55   \n",
       "230409                   sub99          Sub84        QID2744       55   \n",
       "230410                   sub99          Sub84            NaN       55   \n",
       "230411                   sub99          Sub84            NaN       55   \n",
       "\n",
       "        Word_Number  Sentence_Number  Word_In_Sentence_Number        Word   \n",
       "0               2.0              1.0                      2.0         are  \\\n",
       "1               3.0              1.0                      3.0         now   \n",
       "2               4.0              1.0                      4.0   rumblings   \n",
       "3               5.0              1.0                      5.0        that   \n",
       "4               6.0              1.0                      6.0       Apple   \n",
       "...             ...              ...                      ...         ...   \n",
       "230407         58.0              2.0                     34.0  unreliable   \n",
       "230408         59.0              2.0                     35.0         and   \n",
       "230409         60.0              2.0                     36.0  detestable   \n",
       "230410          NaN              NaN                      NaN         NaN   \n",
       "230411          NaN              NaN                      NaN         NaN   \n",
       "\n",
       "       Word_Cleaned  Word_Length  ...  IA_REGRESSION_IN_COUNT   \n",
       "0               are          3.0  ...                     0.0  \\\n",
       "1               now          3.0  ...                     0.0   \n",
       "2         rumblings          9.0  ...                     0.0   \n",
       "3              that          4.0  ...                     NaN   \n",
       "4             apple          5.0  ...                     0.0   \n",
       "...             ...          ...  ...                     ...   \n",
       "230407   unreliable         10.0  ...                     1.0   \n",
       "230408          and          3.0  ...                     NaN   \n",
       "230409   detestable         10.0  ...                     0.0   \n",
       "230410          NaN          NaN  ...                     0.0   \n",
       "230411          NaN          NaN  ...                     0.0   \n",
       "\n",
       "        IA_REGRESSION_OUT  IA_REGRESSION_OUT_COUNT  IA_REGRESSION_OUT_FULL   \n",
       "0                     0.0                      0.0                     0.0  \\\n",
       "1                     0.0                      0.0                     0.0   \n",
       "2                     0.0                      0.0                     0.0   \n",
       "3                     NaN                      NaN                     NaN   \n",
       "4                     0.0                      0.0                     0.0   \n",
       "...                   ...                      ...                     ...   \n",
       "230407                0.0                      0.0                     0.0   \n",
       "230408                NaN                      NaN                     NaN   \n",
       "230409                0.0                      0.0                     0.0   \n",
       "230410                0.0                      0.0                     0.0   \n",
       "230411                1.0                      1.0                     1.0   \n",
       "\n",
       "        IA_REGRESSION_OUT_FULL_COUNT IA_REGRESSION_PATH_DURATION   \n",
       "0                                0.0                       147.0  \\\n",
       "1                                0.0                       193.0   \n",
       "2                                0.0                       198.0   \n",
       "3                                NaN                         NaN   \n",
       "4                                0.0                       235.0   \n",
       "...                              ...                         ...   \n",
       "230407                           0.0                       330.0   \n",
       "230408                           NaN                         NaN   \n",
       "230409                           0.0                       181.0   \n",
       "230410                           0.0                       145.0   \n",
       "230411                           1.0                      1193.0   \n",
       "\n",
       "        IA_FIRST_SACCADE_AMPLITUDE  IA_FIRST_SACCADE_ANGLE   \n",
       "0                             2.39                    9.31  \\\n",
       "1                             1.86                    1.89   \n",
       "2                             2.17                    2.46   \n",
       "3                              NaN                     NaN   \n",
       "4                             3.89                    0.66   \n",
       "...                            ...                     ...   \n",
       "230407                        3.19                    3.73   \n",
       "230408                         NaN                     NaN   \n",
       "230409                        5.38                    8.19   \n",
       "230410                        4.01                   -3.07   \n",
       "230411                        4.17                   -3.68   \n",
       "\n",
       "       IA_FIRST_SACCADE_END_TIME IA_FIRST_SACCADE_START_TIME  \n",
       "0                          247.0                       221.0  \n",
       "1                          415.0                       395.0  \n",
       "2                          632.0                       609.0  \n",
       "3                            NaN                         NaN  \n",
       "4                          864.0                       831.0  \n",
       "...                          ...                         ...  \n",
       "230407                    8728.0                      8702.0  \n",
       "230408                       NaN                         NaN  \n",
       "230409                    9109.0                      9076.0  \n",
       "230410                     220.0                       193.0  \n",
       "230411                    9321.0                      9291.0  \n",
       "\n",
       "[230412 rows x 63 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provo_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first-pass-regression-out\n",
      "IA_REGRESSION_OUT\n",
      "0.0    129221\n",
      "1.0     24345\n",
      "Name: count, dtype: int64\n",
      "NaNs:  76846\n",
      "\n",
      "first pass skips\n",
      "Counter({0: 128807, 1: 101605})\n",
      "NaNs:  0\n"
     ]
    }
   ],
   "source": [
    "for column, measure_type in provo_measures.items():\n",
    "    print(f'\\n{measure_type.name}')\n",
    "    print(provo_raw[column].value_counts())\n",
    "    print('NaNs: ', provo_raw[column].isna().sum())\n",
    "\n",
    "print('\\nfirst pass skips')\n",
    "print(Counter(provo_raw['IA_SKIP']))\n",
    "print('NaNs: ', provo_raw['IA_SKIP'].isna().sum())\n",
    "\n",
    "assert set(provo_raw['IA_REGRESSION_OUT'].dropna().unique()) == set([0, 1])\n",
    "assert set(provo_raw['IA_SKIP'].unique()) == set([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression variable contains several NaNs. Skips are always either 0 or 1. Let's investigate why we have so many ```NaN```s. It seems to be the same case of the dot in RastrOS: if the word was fixated at any point, IA_REGRESSION_OUT is either 0 or 1. When it is NaN, the token was skipped in the first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in provo_raw.iterrows():\n",
    "    regression_label = row['IA_REGRESSION_OUT']\n",
    "    assert regression_label in (0, 1) or pd.isna(regression_label)\n",
    "    if regression_label in (0, 1):\n",
    "        if row['IA_SKIP'] == 1.:\n",
    "            # tokens skipped at first pass but fixated later \n",
    "            assert row['IA_FIXATION_COUNT'] > 0\n",
    "            # strage cases where it was skipped but regression is one,#\n",
    "            # probably because of recursive regressions\n",
    "            #assert regression_label == 0\n",
    "        # if there is a label, the token was fixated at least once\n",
    "        assert row['IA_DWELL_TIME'] > 0.\n",
    "        # checking if first run fixation count is more reliable\n",
    "        assert not pd.isna(row['IA_FIRST_RUN_FIXATION_COUNT'])\n",
    "        assert row['IA_FIRST_RUN_FIXATION_COUNT'] >= 1.\n",
    "    if pd.isna(regression_label):\n",
    "        # the dots mean skipped at first pass and also skipped altogether\n",
    "        assert row['IA_SKIP'] == 1.\n",
    "        assert row['IA_DWELL_TIME'] == 0.\n",
    "        assert row['IA_FIXATION_COUNT'] == 0.\n",
    "        assert pd.isna(row['IA_FIRST_RUN_FIXATION_COUNT'])\n",
    "    if pd.isna(row['IA_FIRST_RUN_FIXATION_COUNT']):\n",
    "        # when it is NaN, regression is also NaN\n",
    "        assert row['IA_SKIP'] == 1.\n",
    "        assert pd.isna(regression_label)\n",
    "    else:\n",
    "        # otherwise regression has a value\n",
    "        assert regression_label in (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when ```IA_REGRESSION_OUT``` is nan, skip is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo_identifiers = {TOKEN_ID.format(text_id, word_id): word for text_id, words in provo_texts.items() for word_id, word in words.items()}\n",
    "provo_subject_ids = {subj_id: i for i, subj_id in enumerate(provo_raw['Participant_ID'].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the tokens in 'Word' and 'IA_LABEL' match and also if they match the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent_words = {}\n",
    "internal_inconsistencies = {}\n",
    "\n",
    "for index, row in provo_raw.iterrows():\n",
    "    text_id = row['Text_ID']\n",
    "    word_id = row['IA_ID']\n",
    "    identifier = TOKEN_ID.format(provo_text_ids[str(text_id)], word_id - 1)\n",
    "    \n",
    "    # check that different subjects identifiers always map to the same word\n",
    "    if provo_identifiers[identifier] != row['IA_LABEL'].strip():\n",
    "        inconsistent_words[index] = (provo_identifiers[identifier], row['IA_LABEL'])\n",
    "\n",
    "    if row['Word'] != row['IA_LABEL']:\n",
    "        internal_inconsistencies[index] = (row['Word'], row['IA_LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1366 tokens with mismatches between IA_LABEL and Word!\n",
      "There are 4 tokens with mismatches wrt the texts!\n",
      "{('Ñ', '? '), ('doesnÕt', 'doesn?t '), ('bondsÕ', 'bonds? '), ('womenÕs', 'women?s ')}\n"
     ]
    }
   ],
   "source": [
    "n_mismatches = len(set(internal_inconsistencies.values()))\n",
    "print(f'There are {n_mismatches} tokens with mismatches between IA_LABEL and Word!')\n",
    "\n",
    "n_mismatches = len(set(inconsistent_words.values()))\n",
    "print(f'There are {n_mismatches} tokens with mismatches wrt the texts!')\n",
    "\n",
    "print(set(inconsistent_words.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot trust the ```Word``` and ```Word_ID``` fields. They have too many inconsistencies and strange NaNs where there shouldn't be. We'll rely on the ```IA_LABEL``` and ```IA_ID``` instead, which seem to be as they should, except for some quotation marks (that we removed a few cells up) and the 4 tokens above, which we'll keep as in the original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_provo_measures(measure, subjects, identifiers, data, replacer):\n",
    "    \"\"\"Create dictionary with measure for each subject in ProVo data.\"\"\"\n",
    "    # initialize the dictionary with np.nans, so that all identifiers have\n",
    "    # all subject keys, even when no data was collected for them\n",
    "    measures = {identifier: {subject: MISSING_LABEL for subject in subjects.values()} for identifier in identifiers.keys()}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        orig_text = str(row['Text_ID'])\n",
    "        text_id = provo_text_ids[orig_text]\n",
    "        token_number = row['IA_ID'] - 1\n",
    "        identifier = TOKEN_ID.format(text_id, token_number)\n",
    "        assert (identifiers[identifier] == row['IA_LABEL'].strip() \n",
    "                or index in inconsistent_words)\n",
    "\n",
    "        orig_subject = row['Participant_ID']\n",
    "        subject = subjects[orig_subject]\n",
    "        \n",
    "        measure_value = row[measure]\n",
    "        was_skipped = row['IA_SKIP']\n",
    "        assert was_skipped  in (0., 1.)\n",
    "\n",
    "        if was_skipped == 1. and replacer is not None:\n",
    "            measure_value = replacer\n",
    "        else:\n",
    "            measure_value = float(measure_value)\n",
    "\n",
    "        # each observation should be unique, otherwise there is a problem in the data\n",
    "        assert np.isnan(measures[identifier][subject])\n",
    "        measures[identifier][subject] = measure_value\n",
    "\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta(PROVO_NAME, provo_texts, provo_text_ids, provo_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix order of the subjects across dataframes\n",
    "provo_ordered_subjs = list(provo_subject_ids.values())\n",
    "\n",
    "def provo_build_and_save(column_name, variable_name, replacer):\n",
    "    measures = get_provo_measures(column_name, provo_subject_ids, provo_identifiers, provo_raw, replacer)\n",
    "    preproc = create_dataframe(variable_name, measures, provo_identifiers, provo_ordered_subjs)\n",
    "    save_preprocessed(PROVO_NAME, variable_name, preproc)\n",
    "    return preproc\n",
    "\n",
    "# first pass regressions, binary -- replace np.nan with skip value\n",
    "provo_fpregs = provo_build_and_save('IA_REGRESSION_OUT', FPREGOUT, replacer=SKIP_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier          0\n",
       "Token               0\n",
       "fpregout:Subj_0     0\n",
       "fpregout:Subj_1     0\n",
       "fpregout:Subj_2     0\n",
       "                   ..\n",
       "fpregout:Subj_79    0\n",
       "fpregout:Subj_80    0\n",
       "fpregout:Subj_81    0\n",
       "fpregout:Subj_82    0\n",
       "fpregout:Subj_83    0\n",
       "Length: 86, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provo_fpregs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Token</th>\n",
       "      <th>fpregout:Subj_0</th>\n",
       "      <th>fpregout:Subj_1</th>\n",
       "      <th>fpregout:Subj_2</th>\n",
       "      <th>fpregout:Subj_3</th>\n",
       "      <th>fpregout:Subj_4</th>\n",
       "      <th>fpregout:Subj_5</th>\n",
       "      <th>fpregout:Subj_6</th>\n",
       "      <th>fpregout:Subj_7</th>\n",
       "      <th>...</th>\n",
       "      <th>fpregout:Subj_74</th>\n",
       "      <th>fpregout:Subj_75</th>\n",
       "      <th>fpregout:Subj_76</th>\n",
       "      <th>fpregout:Subj_77</th>\n",
       "      <th>fpregout:Subj_78</th>\n",
       "      <th>fpregout:Subj_79</th>\n",
       "      <th>fpregout:Subj_80</th>\n",
       "      <th>fpregout:Subj_81</th>\n",
       "      <th>fpregout:Subj_82</th>\n",
       "      <th>fpregout:Subj_83</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_0_token_0</td>\n",
       "      <td>There</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_0_token_1</td>\n",
       "      <td>are</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_0_token_2</td>\n",
       "      <td>now</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_0_token_3</td>\n",
       "      <td>rumblings</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_0_token_4</td>\n",
       "      <td>that</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>text_54_token_55</td>\n",
       "      <td>most</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>text_54_token_56</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>text_54_token_57</td>\n",
       "      <td>and</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>text_54_token_58</td>\n",
       "      <td>detestable</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>text_54_token_59</td>\n",
       "      <td>profession--writing.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2743 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Identifier                 Token  fpregout:Subj_0   \n",
       "0       text_0_token_0                 There              0.0  \\\n",
       "1       text_0_token_1                   are              0.0   \n",
       "2       text_0_token_2                   now              0.0   \n",
       "3       text_0_token_3             rumblings              0.0   \n",
       "4       text_0_token_4                  that             -1.0   \n",
       "...                ...                   ...              ...   \n",
       "2738  text_54_token_55                  most             -1.0   \n",
       "2739  text_54_token_56            unreliable              0.0   \n",
       "2740  text_54_token_57                   and             -1.0   \n",
       "2741  text_54_token_58            detestable              0.0   \n",
       "2742  text_54_token_59  profession--writing.              0.0   \n",
       "\n",
       "      fpregout:Subj_1  fpregout:Subj_2  fpregout:Subj_3  fpregout:Subj_4   \n",
       "0                -1.0              0.0              0.0             -1.0  \\\n",
       "1                -1.0             -1.0              0.0              0.0   \n",
       "2                -1.0              0.0              0.0             -1.0   \n",
       "3                 1.0              1.0             -1.0              1.0   \n",
       "4                -1.0              0.0             -1.0              0.0   \n",
       "...               ...              ...              ...              ...   \n",
       "2738              0.0             -1.0              0.0             -1.0   \n",
       "2739              0.0              1.0              0.0              0.0   \n",
       "2740             -1.0              0.0              0.0             -1.0   \n",
       "2741              0.0              0.0              0.0              0.0   \n",
       "2742              1.0              1.0              1.0              1.0   \n",
       "\n",
       "      fpregout:Subj_5  fpregout:Subj_6  fpregout:Subj_7  ...   \n",
       "0                 0.0              0.0             -1.0  ...  \\\n",
       "1                -1.0             -1.0              0.0  ...   \n",
       "2                 0.0             -1.0             -1.0  ...   \n",
       "3                 0.0              0.0              1.0  ...   \n",
       "4                 0.0              0.0             -1.0  ...   \n",
       "...               ...              ...              ...  ...   \n",
       "2738             -1.0              1.0             -1.0  ...   \n",
       "2739              0.0              0.0              0.0  ...   \n",
       "2740             -1.0             -1.0             -1.0  ...   \n",
       "2741              0.0              0.0              0.0  ...   \n",
       "2742              1.0              0.0              0.0  ...   \n",
       "\n",
       "      fpregout:Subj_74  fpregout:Subj_75  fpregout:Subj_76  fpregout:Subj_77   \n",
       "0                  0.0               0.0               0.0               0.0  \\\n",
       "1                 -1.0               0.0               0.0              -1.0   \n",
       "2                  0.0              -1.0              -1.0               0.0   \n",
       "3                  0.0               1.0               1.0               0.0   \n",
       "4                 -1.0               1.0               0.0              -1.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2738               0.0               0.0               0.0              -1.0   \n",
       "2739              -1.0               0.0               0.0               0.0   \n",
       "2740               0.0              -1.0              -1.0              -1.0   \n",
       "2741              -1.0               0.0               0.0               0.0   \n",
       "2742              -1.0               0.0               0.0               1.0   \n",
       "\n",
       "      fpregout:Subj_78  fpregout:Subj_79  fpregout:Subj_80  fpregout:Subj_81   \n",
       "0                  0.0               0.0               0.0               0.0  \\\n",
       "1                  0.0               0.0              -1.0               0.0   \n",
       "2                  0.0              -1.0              -1.0              -1.0   \n",
       "3                  1.0               0.0              -1.0              -1.0   \n",
       "4                  0.0              -1.0               0.0              -1.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2738               0.0              -1.0              -1.0              -1.0   \n",
       "2739               0.0               0.0               0.0              -1.0   \n",
       "2740              -1.0              -1.0               0.0              -1.0   \n",
       "2741               0.0               0.0               0.0              -1.0   \n",
       "2742               1.0               1.0               0.0               1.0   \n",
       "\n",
       "      fpregout:Subj_82  fpregout:Subj_83  \n",
       "0                  0.0              -1.0  \n",
       "1                 -1.0              -1.0  \n",
       "2                  0.0              -1.0  \n",
       "3                  0.0               0.0  \n",
       "4                  0.0              -1.0  \n",
       "...                ...               ...  \n",
       "2738               0.0               0.0  \n",
       "2739               0.0               0.0  \n",
       "2740              -1.0              -1.0  \n",
       "2741               0.0               0.0  \n",
       "2742               0.0               1.0  \n",
       "\n",
       "[2743 rows x 86 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provo_fpregs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MECO L1 (Multilanguages)\n",
    "\n",
    "Preprocessing the Multilingual MECO L1 corpus (Dutch, English, Estoniar, Finnish, German, Greek, Hebrew, Italia, Korean, Norwegian, Russian, Spanish, Turkish). We'll extract only some of the languages.\n",
    "\n",
    "- 13 languages \n",
    "- 12 texts, 5 translated for each language and 7 just on similar topic\n",
    "- Wikipedia-stly\n",
    "- around 45 subjects for each language\n",
    "- EyeLink Portable Duo, 1000 or 1000+ (SR Research)\n",
    "- Each of the 12 texts appeared on a separate screen\n",
    "\n",
    "\n",
    "We follow the documentation described in the page they point to [here](https://rdrr.io/github/sascha2schroeder/popEye/). Actually, the variables are described [here](https://rdrr.io/github/sascha2schroeder/popEye/f/materials/Measures.md). The main variable we want is ```firstrun.reg.out```: \n",
    "\n",
    "> Variable indicating whether there was a regression from the word during first-pass reading\n",
    "\n",
    "We use ```firstrun.skip```  to detect the tokens that were skipped in the first run:\n",
    "\n",
    "> Variable indicating whether the IA was skipped during first-pass reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MECOL1_NAME = 'mecol1'\n",
    "PATH_TO_MECOL1 = Path('data/MECO-L1/osfstorage/release 1.0/version 1.2/primary data/eye tracking data/joint_data_trimmed.rda')\n",
    "PATH_TO_MECOL1_TEXTS = Path('data/MECO-L1/osfstorage/release 1.0/version 1.2/auxiliary files/reading task materials/supp texts.csv')\n",
    "\n",
    "meco_l1_measures = {\n",
    "    'firstrun.reg.out': FPREGOUT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = rdata.parser.parse_file(PATH_TO_MECOL1)\n",
    "mecol1_raw = rdata.conversion.convert(parsed)['joint.data']\n",
    "\n",
    "# get only Dutch, other languages have inconsistencies\n",
    "mecol1_raw = mecol1_raw[mecol1_raw.lang == 'du']\n",
    "mecol1_texts_raw = pd.read_csv(PATH_TO_MECOL1_TEXTS, index_col=0).loc['Dutch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecol1_text_ids = {text: i for i, text in enumerate(mecol1_texts_raw.index) if 'Unnamed' not in text}\n",
    "mecol1_texts = {i: {} for i in mecol1_text_ids.values()}\n",
    "\n",
    "for index, text in mecol1_texts_raw.items():\n",
    "    if 'Unnamed' not in index:\n",
    "        text_id = mecol1_text_ids[index]\n",
    "        tokens = text.replace('-', '- ')\n",
    "        mecol1_texts[text_id] = {i: word for i, word in enumerate(tokens.split())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking NaNs in both measures. Here, the number of fixations also contain NaNs, exactly the same number as the regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first-pass-regression-out\n",
      "firstrun.reg.out\n",
      "0.0    34748\n",
      "1.0    12054\n",
      "Name: count, dtype: int64\n",
      "NaNs: 20793\n",
      "\n",
      "first run skip\n",
      "firstrun.skip\n",
      "1.0    36058\n",
      "0.0    31537\n",
      "Name: count, dtype: int64\n",
      "NaNs:  0\n"
     ]
    }
   ],
   "source": [
    "for column, measure_type in meco_l1_measures.items():\n",
    "    print(f'\\n{measure_type.name}')\n",
    "    print(mecol1_raw [column].value_counts())\n",
    "    print('NaNs:', mecol1_raw [column].isna().sum())\n",
    "\n",
    "print('\\nfirst run skip')\n",
    "print(mecol1_raw['firstrun.skip'].value_counts())\n",
    "print('NaNs: ', mecol1_raw['firstrun.skip'].isna().sum())\n",
    "\n",
    "assert set(mecol1_raw['firstrun.reg.out'].dropna().unique()) == set([0, 1])\n",
    "assert set(mecol1_raw['firstrun.skip'].unique()) == set([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in mecol1_raw.iterrows():\n",
    "    regression_label = row['firstrun.reg.out']\n",
    "    assert regression_label in (0.0, 1.0) or pd.isna(regression_label)\n",
    "    if regression_label in (0.0, 1.0):\n",
    "        assert row['firstrun.nfix'] > 0.\n",
    "        assert not pd.isna(row['firstrun.nfix'])\n",
    "        assert row['dur'] > 0.\n",
    "    if pd.isna(regression_label):\n",
    "        # nans imply skipped tokens\n",
    "        assert row['firstrun.skip'] == 1.\n",
    "        assert pd.isna(row['firstrun.dur'])\n",
    "        assert pd.isna(row['firstrun.nfix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a regression label, there was a first run fixation. If regression is nan, then first run skip is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in mecol1_raw.iterrows():\n",
    "    label = row['nfix']\n",
    "    if not pd.isna(label):\n",
    "        if row['firstrun.skip'] != 0.:\n",
    "            #print(index)  # Too many!\n",
    "            pass\n",
    "        assert row['dur'] > 0.\n",
    "    else:\n",
    "        assert row['skip'] == 1. or pd.isna(row['skip'])\n",
    "        assert pd.isna(row['dur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for both measures, NaN means the token was not fixated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecol1_identifiers = {TOKEN_ID.format(text_id, word_id): word for text_id, words in mecol1_texts.items() for word_id, word in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecol1_subject_ids = {subj_id: i for i, subj_id in enumerate(mecol1_raw['uniform_id'].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check ```trialid``` is indeed fixed across subjects and encodes the text identifier, despite their strange documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dic = {x: {} for x in range(1, 13)}\n",
    "for index, row in mecol1_raw.iterrows():\n",
    "    # assuming this ID is fixed, although they mention position in experiment in the doc\n",
    "    text_id = row['trialid']\n",
    "    word_id = row['ianum']\n",
    "    word = row['ia']\n",
    "\n",
    "    if word_id not in temp_dic[text_id]:\n",
    "        temp_dic[text_id][word_id] = word\n",
    "    assert temp_dic[text_id][word_id] == word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether it matches the raw texts. Alternatively we can use only the info in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_id, words in temp_dic.items():\n",
    "    for word_id in range(1, len(words)+1):\n",
    "        try:\n",
    "            word = words[word_id]\n",
    "        except KeyError:\n",
    "            print('Investigate...')\n",
    "\n",
    "        if word != mecol1_texts[text_id - 1][word_id - 1]:\n",
    "            print('Mismatch!', word, mecol1_texts[text_id - 1][word_id - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mecol1_measures(measure, subjects, identifiers, data, replacer):\n",
    "    \"\"\"Create dictionary with measure for each subject in MECO-L1 data.\"\"\"\n",
    "    # initialize the dictionary with np.nans, so that all identifiers have\n",
    "    # all subject keys, even when no data was collected for them\n",
    "    measures = {identifier: {subject: MISSING_LABEL for subject in subjects.values()} for identifier in identifiers.keys()}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        text_id = int(row['trialid'] - 1)\n",
    "        token_number = int(row['ianum'] - 1)\n",
    "        identifier = TOKEN_ID.format(text_id, token_number)\n",
    "        assert row['ia'].strip(' ') == mecol1_texts[text_id][token_number]\n",
    "        \n",
    "        subject = mecol1_subject_ids[row['uniform_id']]\n",
    "\n",
    "        measure_value = row[measure]\n",
    "        was_skipped = row['firstrun.skip']\n",
    "        assert was_skipped in (0., 1.)\n",
    "\n",
    "        if was_skipped == 1. and replacer is not None:\n",
    "            measure_value = replacer\n",
    "        else:\n",
    "            measure_value = float(measure_value)\n",
    "\n",
    "        # each observation should be unique, otherwise there is a problem in the data\n",
    "        assert np.isnan(measures[identifier][subject])\n",
    "        measures[identifier][subject] = measure_value\n",
    "            \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta(f'{MECOL1_NAME}_du', mecol1_texts, mecol1_text_ids, mecol1_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix order of the subjects across dataframes\n",
    "mecol1_ordered_subjs = list(mecol1_subject_ids.values())\n",
    "\n",
    "def mecol1_build_and_save(column_name, variable_name, replacer):\n",
    "    measures = get_mecol1_measures(column_name,\n",
    "                                   mecol1_subject_ids,\n",
    "                                   mecol1_identifiers,\n",
    "                                   mecol1_raw,\n",
    "                                   replacer)\n",
    "\n",
    "    preproc = create_dataframe(variable_name,\n",
    "                               measures,\n",
    "                               mecol1_identifiers,\n",
    "                               mecol1_ordered_subjs)\n",
    "    save_preprocessed(f'{MECOL1_NAME}_du', variable_name, preproc)\n",
    "    return preproc\n",
    "\n",
    "mecol1_fpregs = mecol1_build_and_save('firstrun.reg.out', FPREGOUT, replacer=SKIP_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier             0\n",
       "Token                  0\n",
       "fpregout:Subj_0      383\n",
       "fpregout:Subj_1     1103\n",
       "fpregout:Subj_2     1112\n",
       "fpregout:Subj_3     1122\n",
       "fpregout:Subj_4      161\n",
       "fpregout:Subj_5      347\n",
       "fpregout:Subj_6        0\n",
       "fpregout:Subj_7      756\n",
       "fpregout:Subj_8      552\n",
       "fpregout:Subj_9      907\n",
       "fpregout:Subj_10     745\n",
       "fpregout:Subj_11     748\n",
       "fpregout:Subj_12     886\n",
       "fpregout:Subj_13     936\n",
       "fpregout:Subj_14    1123\n",
       "fpregout:Subj_15     920\n",
       "fpregout:Subj_16    1117\n",
       "fpregout:Subj_17     789\n",
       "fpregout:Subj_18     342\n",
       "fpregout:Subj_19       0\n",
       "fpregout:Subj_20    1132\n",
       "fpregout:Subj_21     760\n",
       "fpregout:Subj_22     763\n",
       "fpregout:Subj_23    1325\n",
       "fpregout:Subj_24     173\n",
       "fpregout:Subj_25     730\n",
       "fpregout:Subj_26     511\n",
       "fpregout:Subj_27    1298\n",
       "fpregout:Subj_28     169\n",
       "fpregout:Subj_29    1277\n",
       "fpregout:Subj_30     573\n",
       "fpregout:Subj_31     213\n",
       "fpregout:Subj_32     938\n",
       "fpregout:Subj_33     918\n",
       "fpregout:Subj_34     578\n",
       "fpregout:Subj_35    1112\n",
       "fpregout:Subj_36    1295\n",
       "fpregout:Subj_37     929\n",
       "fpregout:Subj_38     565\n",
       "fpregout:Subj_39     928\n",
       "fpregout:Subj_40    1060\n",
       "fpregout:Subj_41     925\n",
       "fpregout:Subj_42     579\n",
       "fpregout:Subj_43       0\n",
       "fpregout:Subj_44       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecol1_fpregs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Token</th>\n",
       "      <th>fpregout:Subj_0</th>\n",
       "      <th>fpregout:Subj_1</th>\n",
       "      <th>fpregout:Subj_2</th>\n",
       "      <th>fpregout:Subj_3</th>\n",
       "      <th>fpregout:Subj_4</th>\n",
       "      <th>fpregout:Subj_5</th>\n",
       "      <th>fpregout:Subj_6</th>\n",
       "      <th>fpregout:Subj_7</th>\n",
       "      <th>...</th>\n",
       "      <th>fpregout:Subj_35</th>\n",
       "      <th>fpregout:Subj_36</th>\n",
       "      <th>fpregout:Subj_37</th>\n",
       "      <th>fpregout:Subj_38</th>\n",
       "      <th>fpregout:Subj_39</th>\n",
       "      <th>fpregout:Subj_40</th>\n",
       "      <th>fpregout:Subj_41</th>\n",
       "      <th>fpregout:Subj_42</th>\n",
       "      <th>fpregout:Subj_43</th>\n",
       "      <th>fpregout:Subj_44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_0_token_0</td>\n",
       "      <td>Janus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_0_token_1</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_0_token_2</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_0_token_3</td>\n",
       "      <td>de</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_0_token_4</td>\n",
       "      <td>oude</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>text_11_token_164</td>\n",
       "      <td>met</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>text_11_token_165</td>\n",
       "      <td>overheden</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>text_11_token_166</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>text_11_token_167</td>\n",
       "      <td>internationale</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>text_11_token_168</td>\n",
       "      <td>organisaties.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2231 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Identifier           Token  fpregout:Subj_0  fpregout:Subj_1   \n",
       "0        text_0_token_0           Janus              0.0              NaN  \\\n",
       "1        text_0_token_1              is             -1.0              NaN   \n",
       "2        text_0_token_2              in             -1.0              NaN   \n",
       "3        text_0_token_3              de             -1.0              NaN   \n",
       "4        text_0_token_4            oude              1.0              NaN   \n",
       "...                 ...             ...              ...              ...   \n",
       "2226  text_11_token_164             met              0.0              NaN   \n",
       "2227  text_11_token_165       overheden              1.0              NaN   \n",
       "2228  text_11_token_166              en             -1.0              NaN   \n",
       "2229  text_11_token_167  internationale              0.0              NaN   \n",
       "2230  text_11_token_168   organisaties.              0.0              NaN   \n",
       "\n",
       "      fpregout:Subj_2  fpregout:Subj_3  fpregout:Subj_4  fpregout:Subj_5   \n",
       "0                 0.0              NaN              0.0              NaN  \\\n",
       "1                 0.0              NaN             -1.0              NaN   \n",
       "2                -1.0              NaN              1.0              NaN   \n",
       "3                 0.0              NaN             -1.0              NaN   \n",
       "4                -1.0              NaN             -1.0              NaN   \n",
       "...               ...              ...              ...              ...   \n",
       "2226             -1.0             -1.0              0.0             -1.0   \n",
       "2227              0.0              0.0             -1.0              0.0   \n",
       "2228             -1.0             -1.0             -1.0             -1.0   \n",
       "2229              0.0              1.0              0.0              0.0   \n",
       "2230              1.0              1.0              1.0              1.0   \n",
       "\n",
       "      fpregout:Subj_6  fpregout:Subj_7  ...  fpregout:Subj_35   \n",
       "0                -1.0              NaN  ...              -1.0  \\\n",
       "1                -1.0              NaN  ...              -1.0   \n",
       "2                -1.0              NaN  ...              -1.0   \n",
       "3                 1.0              NaN  ...              -1.0   \n",
       "4                 1.0              NaN  ...              -1.0   \n",
       "...               ...              ...  ...               ...   \n",
       "2226              0.0              0.0  ...               NaN   \n",
       "2227             -1.0              0.0  ...               NaN   \n",
       "2228             -1.0             -1.0  ...               NaN   \n",
       "2229              0.0              0.0  ...               NaN   \n",
       "2230              1.0              1.0  ...               NaN   \n",
       "\n",
       "      fpregout:Subj_36  fpregout:Subj_37  fpregout:Subj_38  fpregout:Subj_39   \n",
       "0                  0.0               NaN               NaN               0.0  \\\n",
       "1                  0.0               NaN               NaN              -1.0   \n",
       "2                 -1.0               NaN               NaN              -1.0   \n",
       "3                 -1.0               NaN               NaN              -1.0   \n",
       "4                  0.0               NaN               NaN               1.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2226               NaN              -1.0              -1.0              -1.0   \n",
       "2227               NaN               0.0               0.0              -1.0   \n",
       "2228               NaN              -1.0              -1.0              -1.0   \n",
       "2229               NaN               1.0               0.0              -1.0   \n",
       "2230               NaN               1.0               1.0               0.0   \n",
       "\n",
       "      fpregout:Subj_40  fpregout:Subj_41  fpregout:Subj_42  fpregout:Subj_43   \n",
       "0                 -1.0               0.0               NaN               0.0  \\\n",
       "1                  0.0              -1.0               NaN               1.0   \n",
       "2                 -1.0               1.0               NaN              -1.0   \n",
       "3                  1.0              -1.0               NaN               0.0   \n",
       "4                  0.0               0.0               NaN               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2226               NaN               NaN              -1.0              -1.0   \n",
       "2227               NaN               NaN              -1.0               0.0   \n",
       "2228               NaN               NaN              -1.0              -1.0   \n",
       "2229               NaN               NaN               1.0               0.0   \n",
       "2230               NaN               NaN               1.0               1.0   \n",
       "\n",
       "      fpregout:Subj_44  \n",
       "0                  0.0  \n",
       "1                 -1.0  \n",
       "2                  0.0  \n",
       "3                 -1.0  \n",
       "4                 -1.0  \n",
       "...                ...  \n",
       "2226              -1.0  \n",
       "2227              -1.0  \n",
       "2228              -1.0  \n",
       "2229               1.0  \n",
       "2230               0.0  \n",
       "\n",
       "[2231 rows x 47 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecol1_fpregs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MECO L2\n",
    "Preprocessing the MECO corpus (second language English).\n",
    "\n",
    "- 543 participants with 12 L1 languages (1 is English)\n",
    "- popEye \n",
    "- 12 texts (training materials for the ACCUPLACER Reading test and the English as Second Language Reading Skills test)\n",
    "- participants read 12 texts in their L1 silently for comprehension while their eye movements were recorded, and then answered four yes/no questions after each text.\n",
    "\n",
    "This corpus was released as a ```rda``` files. We use the ```rdata``` library to read it into a pandas dataframe.\n",
    "\n",
    "The documentation of the variables is [here](https://rdrr.io/github/sascha2schroeder/popEye/f/materials/Measures.md). We will use the same variables as MECO L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "meco_l2_measures = {\n",
    "    'firstrun.reg.out': FPREGOUT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/blasota/anaconda3/envs/regressions/lib/python3.9/site-packages/rdata/conversion/_conversion.py:843: UserWarning: Missing constructor for R class \"tbl_df\". The constructor for class \"tbl\" will be used instead.\n",
      "  warnings.warn(\n",
      "/home/users/blasota/anaconda3/envs/regressions/lib/python3.9/site-packages/rdata/conversion/_conversion.py:843: UserWarning: Missing constructor for R class \"tbl\". The constructor for class \"data.frame\" will be used instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MECOL2_NAME = 'mecol2_enl2'\n",
    "\n",
    "PATH_TO_MECOL2 = Path('data/MECO-L2/osfstorage/release 1.0/version 1.1/primary data/eye tracking data/joint_data_l2_trimmed.rda')\n",
    "parsed = rdata.parser.parse_file(PATH_TO_MECOL2)\n",
    "mecol2_raw = rdata.conversion.convert(parsed)['joint.data']\n",
    "\n",
    "PATH_TO_MECOL2_TEXTS = Path('data/MECO-L2/osfstorage/release 1.0/version 1.1/auxiliary files/materials/texts.meco.l2.rda')\n",
    "parsed = rdata.parser.parse_file(PATH_TO_MECOL2_TEXTS)\n",
    "meco_texts_raw = rdata.conversion.convert(parsed)['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecol2_texts = {}\n",
    "mecol2_text_ids = {}\n",
    "\n",
    "for index, row in meco_texts_raw.iterrows():\n",
    "    text_id = row['trialid']\n",
    "    mecol2_text_ids[text_id] = index - 1\n",
    "\n",
    "    text_raw = row['text']\n",
    "    # we need this so that the tokens match what was shown to participants (hyphenated words become two interest areas)\n",
    "    text_tokens = text_raw.replace('-', '- ').split()\n",
    "\n",
    "    mecol2_texts[index - 1] = {i: word for i, word in enumerate(text_tokens)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first-pass-regression-out\n",
      "firstrun.reg.out\n",
      "0.0    407130\n",
      "1.0     97913\n",
      "Name: count, dtype: int64\n",
      "NaNs: 164910\n",
      "\n",
      "first run skip\n",
      "firstrun.skip\n",
      "1.0    36058\n",
      "0.0    31537\n",
      "Name: count, dtype: int64\n",
      "NaNs:  0\n"
     ]
    }
   ],
   "source": [
    "for column, measure_type in meco_l2_measures.items():\n",
    "    print(f'\\n{measure_type.name}')\n",
    "    print(mecol2_raw[column].value_counts())\n",
    "    print('NaNs:', mecol2_raw[column].isna().sum())\n",
    "\n",
    "print('\\nfirst run skip')\n",
    "print(mecol1_raw['firstrun.skip'].value_counts())\n",
    "print('NaNs: ', mecol1_raw['firstrun.skip'].isna().sum())\n",
    "\n",
    "assert set(mecol2_raw['firstrun.reg.out'].dropna().unique()) == set([0, 1])\n",
    "assert set(mecol2_raw['firstrun.skip'].unique()) == set([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the NaNs in the firstrun.reg.out variable. Similar to Provo, always when ```firstrun.reg.out``` is NaN, it was skipped in the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in mecol2_raw.iterrows():\n",
    "    regression_label = row['firstrun.reg.out']\n",
    "    assert regression_label in (0.0, 1.0) or pd.isna(regression_label)\n",
    "    if regression_label in (0.0, 1.0):\n",
    "        assert row['firstrun.nfix'] > 0.\n",
    "        assert not pd.isna(row['firstrun.nfix'])\n",
    "        assert row['dur'] > 0.\n",
    "    if pd.isna(regression_label):\n",
    "        # nan imply skip\n",
    "        assert row['firstrun.skip'] == 1.\n",
    "        assert pd.isna(row['firstrun.dur'])\n",
    "        assert pd.isna(row['firstrun.nfix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in mecol2_raw.iterrows():\n",
    "    label = row['nfix']\n",
    "    if not pd.isna(label):\n",
    "        if row['firstrun.skip'] != 0.:\n",
    "            #print(index)  # Too many!\n",
    "            pass\n",
    "        assert row['dur'] > 0.\n",
    "    else:\n",
    "        assert row['skip'] == 1.\n",
    "        assert pd.isna(row['dur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nfix is nan when the token was skipped. But the number is different from the first pass regression skips, apparently because in the first pass skip there are cases where a word was fixated later (i.e., if we assert row['skip'] == 1. above, it throws an error, but below it does not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecol2_identifiers = {TOKEN_ID.format(text_id, word_id): word for text_id, words in mecol2_texts.items() for word_id, word in words.items()}\n",
    "\n",
    "# ignore subjects who have strangely repeated data\n",
    "EXCLUDED_SUBJS_MECOL2 = ('macmo03', 'macmo06', 'macmo11', 'macmo38', 'macmo39')\n",
    "mecol2_subject_ids = {subj_id: i for i, subj_id in enumerate(mecol2_raw['subid'].unique()) if subj_id not in EXCLUDED_SUBJS_MECOL2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are no inconsistencies between the interest area column and the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent_words = {}\n",
    "\n",
    "for index, row in mecol2_raw.iterrows():\n",
    "    if index > 10000:\n",
    "        # check only up to a point because it's too large\n",
    "        # the final check is also done upon adding the token to the dictionary below\n",
    "        break\n",
    "    text_id = row['itemid']\n",
    "    word_id = row['ianum']\n",
    "    word = mecol2_texts[mecol2_text_ids[int(text_id)]][word_id - 1]\n",
    "\n",
    "    if not row['ia'] == word:\n",
    "        inconsistent_words[index] = (word, row['ia'])\n",
    "\n",
    "assert len(inconsistent_words) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mecol2_measures(measure, subjects, identifiers, data, replacer):\n",
    "    \"\"\"Create dictionary with measure for each subject in MECO-L2 data.\"\"\"\n",
    "    # initialize the dictionary with np.nans, so that all identifiers have\n",
    "    # all subject keys, even when no data was collected for them\n",
    "    measures = {identifier: {subject: MISSING_LABEL for subject in subjects.values()} for identifier in identifiers.keys()} \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        text_id = mecol2_text_ids[float(row['itemid'])]\n",
    "        token_number = int(row['ianum'] - 1)\n",
    "        identifier = TOKEN_ID.format(text_id, token_number)\n",
    "        assert row['ia'].strip(' ') == mecol2_texts[text_id][token_number]\n",
    "        \n",
    "        if row['subid'] in EXCLUDED_SUBJS_MECOL2:\n",
    "            # these subjects have strangely repeated data\n",
    "            continue\n",
    "        subject = subjects[row['subid']]\n",
    "\n",
    "        measure_value = row[measure]\n",
    "        was_skipped = row['firstrun.skip']\n",
    "        assert was_skipped in (0., 1.)\n",
    "\n",
    "        if was_skipped == 1. and replacer is not None:\n",
    "            measure_value = replacer\n",
    "        else:\n",
    "            measure_value = float(measure_value)\n",
    "\n",
    "        # each observation should be unique, otherwise there is a problem in the data\n",
    "        assert np.isnan(measures[identifier][subject])\n",
    "        measures[identifier][subject] = measure_value\n",
    "            \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta(MECOL2_NAME, mecol2_texts, mecol2_text_ids, mecol2_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix order of the subjects across dataframes\n",
    "mecol2_ordered_subjs = list(mecol2_subject_ids.values())\n",
    "\n",
    "def mecol2_build_and_save(column_name, variable_name, replacer):\n",
    "    measures = get_mecol2_measures(column_name,\n",
    "                                   mecol2_subject_ids,\n",
    "                                   mecol2_identifiers,\n",
    "                                   mecol2_raw,\n",
    "                                   replacer)\n",
    "\n",
    "    preproc = create_dataframe(variable_name,\n",
    "                               measures,\n",
    "                               mecol2_identifiers,\n",
    "                               mecol2_ordered_subjs)\n",
    "    save_preprocessed(MECOL2_NAME, variable_name, preproc)\n",
    "    return preproc\n",
    "\n",
    "\n",
    "mecol2_fpregs = mecol2_build_and_save('firstrun.reg.out', FPREGOUT, replacer=SKIP_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier             0\n",
       "Token                  0\n",
       "fpregout:Subj_0      676\n",
       "fpregout:Subj_1      635\n",
       "fpregout:Subj_2       98\n",
       "                    ... \n",
       "fpregout:Subj_538    526\n",
       "fpregout:Subj_539    455\n",
       "fpregout:Subj_540    963\n",
       "fpregout:Subj_541    803\n",
       "fpregout:Subj_542    801\n",
       "Length: 540, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecol2_fpregs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Token</th>\n",
       "      <th>fpregout:Subj_0</th>\n",
       "      <th>fpregout:Subj_1</th>\n",
       "      <th>fpregout:Subj_2</th>\n",
       "      <th>fpregout:Subj_3</th>\n",
       "      <th>fpregout:Subj_4</th>\n",
       "      <th>fpregout:Subj_5</th>\n",
       "      <th>fpregout:Subj_6</th>\n",
       "      <th>fpregout:Subj_7</th>\n",
       "      <th>...</th>\n",
       "      <th>fpregout:Subj_533</th>\n",
       "      <th>fpregout:Subj_534</th>\n",
       "      <th>fpregout:Subj_535</th>\n",
       "      <th>fpregout:Subj_536</th>\n",
       "      <th>fpregout:Subj_537</th>\n",
       "      <th>fpregout:Subj_538</th>\n",
       "      <th>fpregout:Subj_539</th>\n",
       "      <th>fpregout:Subj_540</th>\n",
       "      <th>fpregout:Subj_541</th>\n",
       "      <th>fpregout:Subj_542</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_0_token_0</td>\n",
       "      <td>Samuel</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_0_token_1</td>\n",
       "      <td>Morse,</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_0_token_2</td>\n",
       "      <td>best</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_0_token_3</td>\n",
       "      <td>known</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_0_token_4</td>\n",
       "      <td>today</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>text_11_token_143</td>\n",
       "      <td>their</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>text_11_token_144</td>\n",
       "      <td>connectivity</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>text_11_token_145</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>text_11_token_146</td>\n",
       "      <td>personal</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>text_11_token_147</td>\n",
       "      <td>lives.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1658 rows × 540 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Identifier         Token  fpregout:Subj_0  fpregout:Subj_1   \n",
       "0        text_0_token_0        Samuel              0.0             -1.0  \\\n",
       "1        text_0_token_1        Morse,             -1.0              1.0   \n",
       "2        text_0_token_2          best              1.0             -1.0   \n",
       "3        text_0_token_3         known              0.0              0.0   \n",
       "4        text_0_token_4         today              0.0              0.0   \n",
       "...                 ...           ...              ...              ...   \n",
       "1653  text_11_token_143         their             -1.0              0.0   \n",
       "1654  text_11_token_144  connectivity             -1.0             -1.0   \n",
       "1655  text_11_token_145            in             -1.0             -1.0   \n",
       "1656  text_11_token_146      personal             -1.0             -1.0   \n",
       "1657  text_11_token_147        lives.              1.0              1.0   \n",
       "\n",
       "      fpregout:Subj_2  fpregout:Subj_3  fpregout:Subj_4  fpregout:Subj_5   \n",
       "0                 0.0              NaN              0.0              NaN  \\\n",
       "1                -1.0              NaN              1.0              NaN   \n",
       "2                 1.0              NaN              0.0              NaN   \n",
       "3                -1.0              NaN             -1.0              NaN   \n",
       "4                 1.0              NaN              1.0              NaN   \n",
       "...               ...              ...              ...              ...   \n",
       "1653             -1.0              1.0              0.0              0.0   \n",
       "1654              0.0             -1.0             -1.0              0.0   \n",
       "1655             -1.0              1.0              1.0              0.0   \n",
       "1656              1.0              0.0              1.0              1.0   \n",
       "1657             -1.0              1.0             -1.0              1.0   \n",
       "\n",
       "      fpregout:Subj_6  fpregout:Subj_7  ...  fpregout:Subj_533   \n",
       "0                 0.0              NaN  ...                0.0  \\\n",
       "1                 0.0              NaN  ...                1.0   \n",
       "2                 0.0              NaN  ...                0.0   \n",
       "3                 0.0              NaN  ...                0.0   \n",
       "4                -1.0              NaN  ...                0.0   \n",
       "...               ...              ...  ...                ...   \n",
       "1653              0.0              0.0  ...                NaN   \n",
       "1654              0.0              0.0  ...                NaN   \n",
       "1655             -1.0             -1.0  ...                NaN   \n",
       "1656              0.0              1.0  ...                NaN   \n",
       "1657              1.0             -1.0  ...                NaN   \n",
       "\n",
       "      fpregout:Subj_534  fpregout:Subj_535  fpregout:Subj_536   \n",
       "0                  -1.0                NaN                NaN  \\\n",
       "1                  -1.0                NaN                NaN   \n",
       "2                  -1.0                NaN                NaN   \n",
       "3                  -1.0                NaN                NaN   \n",
       "4                  -1.0                NaN                NaN   \n",
       "...                 ...                ...                ...   \n",
       "1653                0.0                NaN                NaN   \n",
       "1654                0.0                NaN                NaN   \n",
       "1655                0.0                NaN                NaN   \n",
       "1656                0.0                NaN                NaN   \n",
       "1657                1.0                NaN                NaN   \n",
       "\n",
       "      fpregout:Subj_537  fpregout:Subj_538  fpregout:Subj_539   \n",
       "0                   0.0                0.0                0.0  \\\n",
       "1                   1.0                1.0                1.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                  -1.0                0.0                0.0   \n",
       "4                  -1.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "1653                NaN                0.0                NaN   \n",
       "1654                NaN                0.0                NaN   \n",
       "1655                NaN               -1.0                NaN   \n",
       "1656                NaN                0.0                NaN   \n",
       "1657                NaN               -1.0                NaN   \n",
       "\n",
       "      fpregout:Subj_540  fpregout:Subj_541  fpregout:Subj_542  \n",
       "0                  -1.0               -1.0               -1.0  \n",
       "1                   1.0               -1.0                1.0  \n",
       "2                  -1.0               -1.0                0.0  \n",
       "3                   0.0               -1.0                0.0  \n",
       "4                   0.0               -1.0                0.0  \n",
       "...                 ...                ...                ...  \n",
       "1653                NaN               -1.0               -1.0  \n",
       "1654                NaN                1.0               -1.0  \n",
       "1655                NaN                0.0               -1.0  \n",
       "1656                NaN                1.0               -1.0  \n",
       "1657                NaN                1.0                1.0  \n",
       "\n",
       "[1658 rows x 540 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecol2_fpregs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nicenboim's corpus\n",
    "Preprocessing Nicenboim's corpus (Spanish).\n",
    "\n",
    "- 71 participants\n",
    "- EyeLink 1000 \n",
    "- 120+48 items\n",
    "\n",
    "This corpus was released as a ```rda``` files. We use the ```pyreadr``` library to read it into a pandas dataframe (```rdata``` threw some warnings).\n",
    "\n",
    "I could not find documentation anywhere. Nicenboim's could not confirm whether these are the variables we want. He pointed to [this link](https://mran.microsoft.com/snapshot/2014-08-18_0233/web/packages/em2/em2.pdf), but it's not available. So we'll use:\n",
    "\n",
    "- ```fp_reg```: assuming that it refers to first-pass regression like the others\n",
    "\n",
    "Although this corpus has a measure ```skip```, it's unclear whether it refers to first pass skips or not. We will use ```FPRT``` as an auxiliary. Although no documentation is available, the PoTeC documentation has a measure with the same name. We inspected that column and it seems to contain NaNs that should be the skipped in first pass tokens (because the others are numbers > 0).\n",
    "\n",
    "Participants saw sentences with different conditions. It would require some workaround if we wanted to use this corpus. But we can use only the fillers, which seem to have been the same across participants, based on the reverse engineering below. Nicenboim said that the fillers were the same for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['indivET', 'dataET', 'dataexpET'])\n"
     ]
    }
   ],
   "source": [
    "NICENBOIM_NAME = 'nicenboim_es'\n",
    "\n",
    "PATH_TO_NICENBOIM = 'data/Nicenboim/NicenboimEtAl2013ET.Rda'\n",
    "nicenboim_raw = pyreadr.read_r(PATH_TO_NICENBOIM)\n",
    "print(nicenboim_raw.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I understand that 'dataET' contains all sentences (from their experiment, the secondary experiment and the fillers) and 'dataexpET' contain only their experiment. Nicenboim also confirmed that \"dataET includes experimental sentences for other experiments as well that were used as fillers here\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicenboim_raw = nicenboim_raw['dataET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>sentenceid</th>\n",
       "      <th>FFD</th>\n",
       "      <th>FFP</th>\n",
       "      <th>SFD</th>\n",
       "      <th>FPRT</th>\n",
       "      <th>RBRT</th>\n",
       "      <th>TFT</th>\n",
       "      <th>RPD</th>\n",
       "      <th>CRPD</th>\n",
       "      <th>...</th>\n",
       "      <th>x</th>\n",
       "      <th>xbeg</th>\n",
       "      <th>xend</th>\n",
       "      <th>p</th>\n",
       "      <th>pcu</th>\n",
       "      <th>ran</th>\n",
       "      <th>word</th>\n",
       "      <th>wordn</th>\n",
       "      <th>question.acc</th>\n",
       "      <th>question.corrans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rownames</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>728</td>\n",
       "      <td>0.800635</td>\n",
       "      <td>19.039000</td>\n",
       "      <td>Los</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>41</td>\n",
       "      <td>111</td>\n",
       "      <td>756</td>\n",
       "      <td>0.800635</td>\n",
       "      <td>19.039000</td>\n",
       "      <td>pasajeros</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.800635</td>\n",
       "      <td>19.039000</td>\n",
       "      <td>ignoraban</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>321</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>721</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>194</td>\n",
       "      <td>206</td>\n",
       "      <td>763</td>\n",
       "      <td>0.800635</td>\n",
       "      <td>19.039000</td>\n",
       "      <td>si</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>329</td>\n",
       "      <td>178</td>\n",
       "      <td>899</td>\n",
       "      <td>...</td>\n",
       "      <td>246</td>\n",
       "      <td>211</td>\n",
       "      <td>300</td>\n",
       "      <td>697</td>\n",
       "      <td>0.800635</td>\n",
       "      <td>19.039000</td>\n",
       "      <td>solucionaría</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209184</th>\n",
       "      <td>76</td>\n",
       "      <td>169</td>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "      <td>363</td>\n",
       "      <td>363</td>\n",
       "      <td>363</td>\n",
       "      <td>363</td>\n",
       "      <td>363</td>\n",
       "      <td>1869</td>\n",
       "      <td>...</td>\n",
       "      <td>445</td>\n",
       "      <td>404</td>\n",
       "      <td>450</td>\n",
       "      <td>683</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>19.556625</td>\n",
       "      <td>novela</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209349</th>\n",
       "      <td>76</td>\n",
       "      <td>169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1869</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>455</td>\n",
       "      <td>467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>19.556625</td>\n",
       "      <td>la</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209511</th>\n",
       "      <td>76</td>\n",
       "      <td>169</td>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>2168</td>\n",
       "      <td>...</td>\n",
       "      <td>502</td>\n",
       "      <td>472</td>\n",
       "      <td>530</td>\n",
       "      <td>693</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>19.556625</td>\n",
       "      <td>maestra</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209671</th>\n",
       "      <td>76</td>\n",
       "      <td>169</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "      <td>2469</td>\n",
       "      <td>...</td>\n",
       "      <td>544</td>\n",
       "      <td>535</td>\n",
       "      <td>553</td>\n",
       "      <td>671</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>19.556625</td>\n",
       "      <td>de</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209828</th>\n",
       "      <td>76</td>\n",
       "      <td>169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2469</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>558</td>\n",
       "      <td>611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>19.556625</td>\n",
       "      <td>lengua.</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207510 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subj sentenceid  FFD  FFP  SFD FPRT  RBRT  TFT  RPD  CRPD  ...    x   \n",
       "rownames                                                            ...        \n",
       "1           2        156  169    1  169  169   169  169  169   169  ...   33  \\\n",
       "169         2        156  231    1  231  231   231  231  231   400  ...   99   \n",
       "337         2        156  NaN    0    0  NaN     0  NaN  NaN   400  ...  NaN   \n",
       "505         2        156  321    1  321  321   321  321  321   721  ...  204   \n",
       "673         2        156  178    1    0  178   178  329  178   899  ...  246   \n",
       "...       ...        ...  ...  ...  ...  ...   ...  ...  ...   ...  ...  ...   \n",
       "209184     76        169  363    1  363  363   363  363  363  1869  ...  445   \n",
       "209349     76        169  NaN    0    0  NaN     0  NaN  NaN  1869  ...  NaN   \n",
       "209511     76        169  299    1  299  299   299  299  299  2168  ...  502   \n",
       "209671     76        169  301    1  301  301   301  301  301  2469  ...  544   \n",
       "209828     76        169  NaN    0    0  NaN     0  NaN  NaN  2469  ...  NaN   \n",
       "\n",
       "          xbeg  xend    p       pcu        ran          word  wordn   \n",
       "rownames                                                              \n",
       "1           10    36  728  0.800635  19.039000           Los      1  \\\n",
       "169         41   111  756  0.800635  19.039000     pasajeros      2   \n",
       "337        116   189  NaN  0.800635  19.039000     ignoraban      3   \n",
       "505        194   206  763  0.800635  19.039000            si      4   \n",
       "673        211   300  697  0.800635  19.039000  solucionaría      5   \n",
       "...        ...   ...  ...       ...        ...           ...    ...   \n",
       "209184     404   450  683  0.572381  19.556625        novela     10   \n",
       "209349     455   467  NaN  0.572381  19.556625            la     11   \n",
       "209511     472   530  693  0.572381  19.556625       maestra     12   \n",
       "209671     535   553  671  0.572381  19.556625            de     13   \n",
       "209828     558   611  NaN  0.572381  19.556625       lengua.     14   \n",
       "\n",
       "          question.acc  question.corrans  \n",
       "rownames                                  \n",
       "1                  NaN               NaN  \n",
       "169                NaN               NaN  \n",
       "337                NaN               NaN  \n",
       "505                NaN               NaN  \n",
       "673                NaN               NaN  \n",
       "...                ...               ...  \n",
       "209184             1.0                 F  \n",
       "209349             1.0                 F  \n",
       "209511             1.0                 F  \n",
       "209671             1.0                 F  \n",
       "209828             1.0                 F  \n",
       "\n",
       "[207510 rows x 30 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nicenboim_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of reverse engineering to identify sentences that were ***not*** the same across participants, and later exclude them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_texts = {}\n",
    "varying_sentences = set()\n",
    "\n",
    "for index, row in nicenboim_raw.iterrows():\n",
    "    text_id = row['sentenceid']\n",
    "    if text_id not in aux_texts:\n",
    "        aux_texts[text_id] = {}\n",
    "\n",
    "    word = row['word']\n",
    "    word_id = row['wordn']\n",
    "\n",
    "    if word_id not in aux_texts[text_id]:\n",
    "        aux_texts[text_id][word_id] = word\n",
    "    if word != aux_texts[text_id][word_id]:\n",
    "        varying_sentences.add(text_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 out of 168 sentences vary across subjects.\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(varying_sentences)} out of {len(nicenboim_raw[\"sentenceid\"].unique())} sentences vary across subjects.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the texts, i.e. the sentences that were the same across subjects and we will be used. Also check that tokens were the same for all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicenboim_texts = {}\n",
    "nicenboim_text_ids = {}\n",
    "\n",
    "for index, row in nicenboim_raw.iterrows():\n",
    "    text_id = row['sentenceid']\n",
    "    if text_id in varying_sentences:\n",
    "        # we'll not use sentences that vary across subjects\n",
    "        continue\n",
    "    if text_id not in nicenboim_text_ids:\n",
    "        new_id = len(nicenboim_text_ids)\n",
    "        nicenboim_text_ids[text_id] = new_id\n",
    "        nicenboim_texts[new_id] = {}\n",
    "\n",
    "    word_id = row['wordn'] - 1\n",
    "    word = row['word']\n",
    "\n",
    "    text_id = nicenboim_text_ids[text_id]\n",
    "    if word_id not in nicenboim_texts[text_id]:\n",
    "        nicenboim_texts[text_id][word_id] = word\n",
    "    assert word == nicenboim_texts[text_id][word_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the filtered dataframe containing only filler sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillers = list(nicenboim_text_ids.keys())\n",
    "nicenboim_filtered = nicenboim_raw[(nicenboim_raw.sentenceid.isin(fillers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some integers not used as sentence IDs for some reason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "97\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 171):\n",
    "    if str(x) not in fillers and str(x) not in varying_sentences:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any NaNs to handle here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp_reg\n",
      "0.0    50087\n",
      "1.0     6055\n",
      "Name: count, dtype: int64\n",
      "NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "print(nicenboim_filtered['fp_reg'].value_counts())\n",
    "print('NaNs:', nicenboim_filtered['fp_reg'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPRT has no values 0, but it has NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "NaNs: 23263\n"
     ]
    }
   ],
   "source": [
    "print((nicenboim_filtered['FPRT']<1).sum())\n",
    "print('NaNs:', nicenboim_filtered['FPRT'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(nicenboim_raw['fp_reg'].dropna().unique()) == set([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicenboim_identifiers = {TOKEN_ID.format(text_id, word_id): word for text_id, words in nicenboim_texts.items() for word_id, word in words.items()}\n",
    "nicenboim_subject_ids = {subj_id: i for i, subj_id in enumerate(nicenboim_raw['subj'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nicenboim_measures(measure, subjects, identifiers, data, replacer):\n",
    "    \"\"\"Create dictionary with measure for each subject in Nicenboim data.\"\"\"\n",
    "    measures = {identifier: {subject: MISSING_LABEL for subject in nicenboim_subject_ids.values()} for identifier in nicenboim_identifiers}\n",
    "\n",
    "    for index, row in nicenboim_filtered.iterrows():\n",
    "\n",
    "        text_id = nicenboim_text_ids[row['sentenceid']]\n",
    "        token_number = int(row['wordn'] - 1)\n",
    "        identifier = TOKEN_ID.format(text_id, token_number)\n",
    "        assert row['word'] == nicenboim_texts[text_id][token_number]\n",
    "        \n",
    "        subject = subjects[row['subj']]\n",
    "        measure_value = row[measure]\n",
    "\n",
    "        if pd.isna(row['FPRT']) and replacer is not None:\n",
    "            assert measure_value == 0.\n",
    "            measure_value = replacer\n",
    "        else:\n",
    "            assert row['FPRT'] > 0\n",
    "            measure_value = float(measure_value)\n",
    "\n",
    "        assert np.isnan(measures[identifier][subject])\n",
    "        measures[identifier][subject] = measure_value \n",
    "\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta(NICENBOIM_NAME, nicenboim_texts, nicenboim_text_ids, nicenboim_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix order of the subjects across dataframes\n",
    "nicenboim_ordered_subjs = list(nicenboim_subject_ids.values())\n",
    "\n",
    "def nicenboim_build_and_save(column_name, variable_name, replacer):\n",
    "    measures = get_nicenboim_measures(column_name, nicenboim_subject_ids, nicenboim_identifiers, nicenboim_raw, replacer)\n",
    "    preproc = create_dataframe(variable_name, measures, nicenboim_identifiers, nicenboim_ordered_subjs)\n",
    "    save_preprocessed(NICENBOIM_NAME, variable_name, preproc)\n",
    "    return preproc\n",
    "\n",
    "nicenboim_fpregs = nicenboim_build_and_save('fp_reg', FPREGOUT, replacer=SKIP_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier           0\n",
       "Token                0\n",
       "fpregout:Subj_0      0\n",
       "fpregout:Subj_1     19\n",
       "fpregout:Subj_2      0\n",
       "                    ..\n",
       "fpregout:Subj_66     0\n",
       "fpregout:Subj_67     0\n",
       "fpregout:Subj_68     0\n",
       "fpregout:Subj_69     0\n",
       "fpregout:Subj_70     0\n",
       "Length: 73, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nicenboim_fpregs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Token</th>\n",
       "      <th>fpregout:Subj_0</th>\n",
       "      <th>fpregout:Subj_1</th>\n",
       "      <th>fpregout:Subj_2</th>\n",
       "      <th>fpregout:Subj_3</th>\n",
       "      <th>fpregout:Subj_4</th>\n",
       "      <th>fpregout:Subj_5</th>\n",
       "      <th>fpregout:Subj_6</th>\n",
       "      <th>fpregout:Subj_7</th>\n",
       "      <th>...</th>\n",
       "      <th>fpregout:Subj_61</th>\n",
       "      <th>fpregout:Subj_62</th>\n",
       "      <th>fpregout:Subj_63</th>\n",
       "      <th>fpregout:Subj_64</th>\n",
       "      <th>fpregout:Subj_65</th>\n",
       "      <th>fpregout:Subj_66</th>\n",
       "      <th>fpregout:Subj_67</th>\n",
       "      <th>fpregout:Subj_68</th>\n",
       "      <th>fpregout:Subj_69</th>\n",
       "      <th>fpregout:Subj_70</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_0_token_0</td>\n",
       "      <td>El</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_0_token_1</td>\n",
       "      <td>boxeador</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_0_token_2</td>\n",
       "      <td>anunció</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_0_token_3</td>\n",
       "      <td>que</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_0_token_4</td>\n",
       "      <td>se</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>text_47_token_11</td>\n",
       "      <td>había</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>text_47_token_12</td>\n",
       "      <td>derrotado</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>text_47_token_13</td>\n",
       "      <td>a</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>text_47_token_14</td>\n",
       "      <td>Franco</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>text_47_token_15</td>\n",
       "      <td>Alonso.</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Identifier      Token  fpregout:Subj_0  fpregout:Subj_1   \n",
       "0      text_0_token_0         El              0.0              0.0  \\\n",
       "1      text_0_token_1   boxeador              0.0              0.0   \n",
       "2      text_0_token_2    anunció             -1.0              0.0   \n",
       "3      text_0_token_3        que              0.0             -1.0   \n",
       "4      text_0_token_4         se             -1.0             -1.0   \n",
       "..                ...        ...              ...              ...   \n",
       "786  text_47_token_11      había              0.0              0.0   \n",
       "787  text_47_token_12  derrotado              0.0              0.0   \n",
       "788  text_47_token_13          a             -1.0             -1.0   \n",
       "789  text_47_token_14     Franco              0.0              0.0   \n",
       "790  text_47_token_15    Alonso.             -1.0              0.0   \n",
       "\n",
       "     fpregout:Subj_2  fpregout:Subj_3  fpregout:Subj_4  fpregout:Subj_5   \n",
       "0               -1.0             -1.0              0.0              0.0  \\\n",
       "1                0.0              0.0              0.0              0.0   \n",
       "2                0.0              0.0              0.0              0.0   \n",
       "3               -1.0              0.0             -1.0             -1.0   \n",
       "4                0.0             -1.0             -1.0             -1.0   \n",
       "..               ...              ...              ...              ...   \n",
       "786             -1.0              0.0             -1.0             -1.0   \n",
       "787              1.0              0.0              0.0              0.0   \n",
       "788             -1.0             -1.0             -1.0             -1.0   \n",
       "789              0.0              0.0              0.0              0.0   \n",
       "790              0.0              1.0              0.0              1.0   \n",
       "\n",
       "     fpregout:Subj_6  fpregout:Subj_7  ...  fpregout:Subj_61   \n",
       "0                0.0              0.0  ...              -1.0  \\\n",
       "1                1.0              0.0  ...               0.0   \n",
       "2                0.0             -1.0  ...               0.0   \n",
       "3                0.0              0.0  ...              -1.0   \n",
       "4                0.0              0.0  ...              -1.0   \n",
       "..               ...              ...  ...               ...   \n",
       "786             -1.0             -1.0  ...               0.0   \n",
       "787              1.0              0.0  ...               0.0   \n",
       "788             -1.0             -1.0  ...              -1.0   \n",
       "789              1.0              0.0  ...               0.0   \n",
       "790             -1.0             -1.0  ...              -1.0   \n",
       "\n",
       "     fpregout:Subj_62  fpregout:Subj_63  fpregout:Subj_64  fpregout:Subj_65   \n",
       "0                -1.0               0.0              -1.0              -1.0  \\\n",
       "1                 0.0               0.0               0.0               0.0   \n",
       "2                -1.0               0.0               0.0               0.0   \n",
       "3                 1.0              -1.0               0.0               0.0   \n",
       "4                 0.0               0.0              -1.0              -1.0   \n",
       "..                ...               ...               ...               ...   \n",
       "786              -1.0              -1.0               0.0              -1.0   \n",
       "787               1.0               0.0               0.0               0.0   \n",
       "788              -1.0              -1.0               0.0              -1.0   \n",
       "789               0.0               0.0              -1.0               0.0   \n",
       "790               0.0               0.0              -1.0               0.0   \n",
       "\n",
       "     fpregout:Subj_66  fpregout:Subj_67  fpregout:Subj_68  fpregout:Subj_69   \n",
       "0                -1.0              -1.0               0.0              -1.0  \\\n",
       "1                 0.0               0.0               0.0              -1.0   \n",
       "2                 0.0               0.0               0.0               1.0   \n",
       "3                -1.0               0.0               0.0               0.0   \n",
       "4                -1.0              -1.0               0.0              -1.0   \n",
       "..                ...               ...               ...               ...   \n",
       "786              -1.0               0.0               0.0              -1.0   \n",
       "787               1.0               0.0               0.0               0.0   \n",
       "788               0.0              -1.0              -1.0              -1.0   \n",
       "789               0.0               0.0               0.0               0.0   \n",
       "790               0.0               1.0               0.0               0.0   \n",
       "\n",
       "     fpregout:Subj_70  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 1.0  \n",
       "3                -1.0  \n",
       "4                 0.0  \n",
       "..                ...  \n",
       "786               0.0  \n",
       "787               0.0  \n",
       "788              -1.0  \n",
       "789               0.0  \n",
       "790               0.0  \n",
       "\n",
       "[791 rows x 73 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nicenboim_fpregs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regressions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
