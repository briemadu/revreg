{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Incremental Outputs\n",
    "\n",
    "Here we use bidirectional models to incrementally perform sequence labelling in the reading corpora texts.\n",
    "\n",
    "We have checked what each participant actually saw, sentences or full texts, and feed the same to the models, even though it might differ from the training setting.\n",
    "According to the papers, they saw 'texts', as encoded in the identifiers. For Nicenboim, these are actually sentences.\n",
    "\n",
    "We extract (convenient, effective and normal) revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "\n",
    "import flair\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import stanza\n",
    "import torch\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODEL_DATA = Path('preprocessed/model_data/')\n",
    "PATH_MODEL_OUTPUTS = Path('preprocessed/model_outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rastros_ptbr Egito usava\n",
      "rastros_ptbr fragrância à\n"
     ]
    }
   ],
   "source": [
    "corpora = ['rastros_ptbr', 'potec_de', 'provo_en', 'nicenboim_es', 'mecol1_du', 'mecol2_enl2']\n",
    "\n",
    "texts = {}\n",
    "\n",
    "for corpus in corpora:\n",
    "    PATH_TEXTS = Path(f'preprocessed/texts/{corpus}.json')\n",
    "    with open(PATH_TEXTS, 'r') as file:\n",
    "        texts[corpus] = json.load(file)\n",
    "    # replace two cases where encoding left a special character\n",
    "    for text_id, text_dic in texts[corpus].items():\n",
    "        for token_id, token in text_dic.items():\n",
    "            if '\\xa0' in token:\n",
    "                print(corpus, token)\n",
    "                texts[corpus][text_id][token_id] = token.replace(u'\\xa0', u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no revision occurred, what label to use for convenient and effective (0 or NaN)\n",
    "NA_LABEL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_revision(previous, current):\n",
    "    \"\"\"Return 1 if a revision occurred, else 0.\"\"\"\n",
    "    assert len(previous) == len(current)\n",
    "    if previous == current:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "           \n",
    "def is_convenient_revision(previous, current, final):\n",
    "    \"\"\"Return 1 if a convenient revision occurred, else 0.\"\"\"\n",
    "    assert len(previous) == len(final)\n",
    "    if is_revision(previous, current) == 0:\n",
    "        return NA_LABEL\n",
    "    if previous == final:\n",
    "        return 0\n",
    "    return 1          \n",
    "\n",
    "           \n",
    "def is_effective_revision(previous, current, final):\n",
    "    \"\"\"Return 1 if an effective revision occurred, else 0.\"\"\"\n",
    "    assert len(previous) == len(final) and len(final) == len(current)\n",
    "    if is_revision(previous, current) == 0:\n",
    "        return NA_LABEL\n",
    "    n_correct_before = sum(np.array(previous) == np.array(final))\n",
    "    n_correct_after = sum(np.array(current) == np.array(final)) \n",
    "    if n_correct_after <= n_correct_before:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def get_partial_input(tokens, i):\n",
    "    \"\"\"Get prefix up to token i.\"\"\"\n",
    "    return \" \".join([tokens[str(j)] for j in range(0, i+1)])\n",
    "\n",
    "\n",
    "def get_prefixes(outputs, text_idx, i):\n",
    "    \"\"\"Return previous output, current prefix and final prefix up to label i.\"\"\"\n",
    "    previous_prefix = outputs[text_idx][i-1]\n",
    "    # sometimes one interest area has more than one token (e.g. due to punctuation)\n",
    "    # this will result in the current output prefix being extended with more \n",
    "    # than one label in one time step\n",
    "    # still, we use only the length of the previous prefix for comparison\n",
    "    # and consider everything that was added as 'one label' here\n",
    "    previous_len = len(previous_prefix)\n",
    "    current_prefix = outputs[text_idx][i][:previous_len]\n",
    "    final_prefix = outputs[text_idx][-1][:previous_len]\n",
    "    return previous_prefix, current_prefix, final_prefix\n",
    "\n",
    "\n",
    "def index_df(df, texts):\n",
    "    \"\"\"Fill the Token column in a dataframe with tokens.\"\"\"\n",
    "    # add tokens to the df, the rest will be filled cell by cell\n",
    "    for text_idx, tokens in texts.items():\n",
    "        for i, token in tokens.items():\n",
    "            df.loc[f'text_{text_idx}_token_{i}']['Token'] = token\n",
    "\n",
    "\n",
    "def initialise_df(texts, tasks):\n",
    "    \"\"\"Create a dataframe with the standard structure.\"\"\"\n",
    "    columns = ['Token'] + [f'{prefix}revision:{name}' for name in tasks for prefix in ['', 'convenient-', 'effective-']]\n",
    "    index = [f'text_{text_idx}_token_{i}' for text_idx, tokens in texts.items() for i in tokens]\n",
    "    revisions = pd.DataFrame(columns=columns, index=index)\n",
    "    index_df(revisions, texts)\n",
    "    return revisions\n",
    "\n",
    "\n",
    "def get_revised_signal(outputs, text_idx, i):\n",
    "    \"\"\"Return the labels for the revision dataframe.\"\"\"\n",
    "    # the first token by definition does not cause a revision\n",
    "    if i == 0:\n",
    "        revised, conv_revised, effec_revised = 0, NA_LABEL, NA_LABEL\n",
    "    else:\n",
    "        # check whether/which revisions occurred\n",
    "        previous, current, final = get_prefixes(outputs, text_idx, i)\n",
    "        revised = is_revision(previous, current)\n",
    "        conv_revised = is_convenient_revision(previous, current, final)\n",
    "        effec_revised = is_effective_revision(previous, current, final)\n",
    "    return revised, conv_revised, effec_revised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explosion Pretrained Transformers\n",
    "\n",
    "We extract outputs from the pretrained models at [Explosion's model hub in Hugging Face](https://huggingface.co/explosion). They are described in [this blogpost](https://explosion.ai/blog/ud-benchmarks-v3-2), where it says that:\n",
    "\n",
    "> Aside from the tokenizer, the pipeline components are trained with a single transformer component using xlm-roberta-base, similar to Trankit Base. [...] The tokenizer is trained separately and the remaining components are trained sharing the same transformer component using multi-task learning.\n",
    "\n",
    "They say these models are only meant for benchmarking purposes, but given that they are comparable for many languages, we'll inspect their incremental outputs for the available tasks:\n",
    "\n",
    "- POS-tagging with XPOS (UPOS does not seem to be available for all languages)\n",
    "- Dependency Parsing (task of predicting heads and task of predicting the relations)\n",
    "\n",
    "We use the token annotation documentation from [Spacy](https://spacy.io/api/token) to retrieve the labels.\n",
    "\n",
    "The model cards of the models we use are:\n",
    "\n",
    "- [in Portuguese](https://huggingface.co/explosion/pt_udv25_portuguesebosque_trf)\n",
    "- [in German](https://huggingface.co/explosion/de_udv25_germanhdt_trf)\n",
    "- [in English](https://huggingface.co/explosion/en_udv25_englishewt_trf)\n",
    "- [in Spanish](https://huggingface.co/explosion/es_udv25_spanishancora_trf)\n",
    "- [in Dutch](https://huggingface.co/explosion/nl_udv25_dutchalpino_trf)\n",
    "\n",
    "There are also the [Spacy's models](https://spacy.io/models), but Transformers are not available for all languages. The repository is [here](https://github.com/explosion/spacy-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family = 'hf-trf'\n",
    "\n",
    "corpus_model_names = [\n",
    "    ('rastros_ptbr', 'pt_udv25_portuguesebosque_trf'),\n",
    "    ('potec_de', 'de_udv25_germanhdt_trf'),\n",
    "    ('provo_en', 'en_udv25_englishewt_trf'),\n",
    "    ('nicenboim_es', 'es_udv25_spanishancora_trf'),\n",
    "    ('mecol1_du', 'nl_udv25_dutchalpino_trf'),\n",
    "    ('mecol2_enl2', 'en_udv25_englishewt_trf')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TASKS = ['pos', 'deprel', 'head']\n",
    "\n",
    "def get_outputs(seq, attribute):\n",
    "    \"\"\"Return the label prefix.\"\"\"\n",
    "    return [getattr(token, attribute) for token in seq]\n",
    "\n",
    "\n",
    "def get_head(seq):\n",
    "    \"\"\"Return the heads prefix.\"\"\"\n",
    "    return [str(token.head.i) for token in seq]\n",
    "\n",
    "\n",
    "def create_hf_outputs_dic(corpus_name):\n",
    "    \"\"\"Initialise outputs dictionary to be filled with a list of increasing prefixes.\"\"\"\n",
    "    return {task: {idx: [] for idx in texts[corpus_name]} for task in HF_TASKS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:35<00:00,  1.91s/it]\n",
      "100%|██████████| 12/12 [01:43<00:00,  8.62s/it]\n",
      "100%|██████████| 55/55 [01:44<00:00,  1.89s/it]\n",
      "100%|██████████| 48/48 [00:26<00:00,  1.80it/s]\n",
      "100%|██████████| 12/12 [02:06<00:00, 10.56s/it]\n",
      "100%|██████████| 12/12 [01:22<00:00,  6.86s/it]\n"
     ]
    }
   ],
   "source": [
    "for corpus_name, model_name in corpus_model_names:\n",
    "\n",
    "    outputs = create_hf_outputs_dic(corpus_name)\n",
    "    revisions = initialise_df(texts[corpus_name], HF_TASKS)\n",
    "    model = spacy.load(model_name)\n",
    "\n",
    "    for text_idx, tokens in tqdm(texts[corpus_name].items()):\n",
    "        # first, get the sequence of partial outputs\n",
    "        for i in range(len(tokens)):\n",
    "            partial_input = get_partial_input(tokens, i)\n",
    "            parsed = model(partial_input)\n",
    "            \n",
    "            outputs_pos = get_outputs(parsed, 'pos_')\n",
    "            outputs['pos'][text_idx].append(outputs_pos)\n",
    "            \n",
    "            outputs_deprel = get_outputs(parsed, 'dep_')\n",
    "            outputs['deprel'][text_idx].append(outputs_deprel)\n",
    "\n",
    "            outputs_head = get_head(parsed)\n",
    "            outputs['head'][text_idx].append(outputs_head)\n",
    "                 \n",
    "        # now, loop over the sequence of partial outputs and check for revisions and edits\n",
    "        for task in HF_TASKS:\n",
    "            for i in range(len(tokens)):\n",
    "                # fill in the revisions dataframe \n",
    "                revised, conv_revised, effec_revised = get_revised_signal(outputs[task], text_idx, i)                                         \n",
    "                identifier = f'text_{text_idx}_token_{i}'\n",
    "                revisions.loc[identifier][f'revision:{task}'] = revised\n",
    "                revisions.loc[identifier][f'convenient-revision:{task}'] = conv_revised\n",
    "                revisions.loc[identifier][f'effective-revision:{task}'] = effec_revised\n",
    "\n",
    "    # save everything\n",
    "    revisions.to_csv(PATH_MODEL_DATA / f'{corpus_name}_{model_family}_revisions.tsv', sep='\\t')\n",
    "    for task in HF_TASKS:\n",
    "        with open(PATH_MODEL_OUTPUTS / f'{corpus_name}_{model_family}_{task}.json', 'w') as file:\n",
    "            json.dump(outputs[task], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza Pretrained BiLSTM\n",
    "\n",
    "Stanza's models use BiLSTMs, according to [their paper](https://aclanthology.org/2020.acl-demos.14/). Their repository is [here](https://github.com/stanfordnlp/stanza) and the models are listed [here](https://huggingface.co/stanfordnlp). The official website is [here](https://stanfordnlp.github.io/stanza/). Tutorials showing how to get each type of token annotation are [here](https://stanfordnlp.github.io/stanza/tutorials.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_cpu()\n",
    "\n",
    "#stanza.download('en') \n",
    "#stanza.download('pt') \n",
    "#stanza.download('es') \n",
    "#stanza.download('nl') \n",
    "#stanza.download(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family = 'stanza-bilstm'\n",
    "\n",
    "all_tasks = ['upos', 'xpos', 'ner', 'deprel', 'head']\n",
    "pt_tasks = ['upos', 'deprel', 'head']\n",
    "\n",
    "corpus_model_names = [\n",
    "    ('rastros_ptbr', 'pt', pt_tasks),\n",
    "    ('potec_de', 'de', all_tasks),\n",
    "    ('provo_en', 'en', all_tasks),\n",
    "    ('nicenboim_es', 'es', all_tasks),\n",
    "    ('mecol1_du', 'nl', all_tasks),\n",
    "    ('mecol2_enl2', 'en', all_tasks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stanza_outputs(doc, attribute):\n",
    "    if attribute == 'ner':\n",
    "        return [str(getattr(token, attribute)) for sent in doc.sentences for token in sent.tokens]\n",
    "    return [str(getattr(token, attribute)) for sent in doc.sentences for token in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:16:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41213097cf864293bf788f6234ae419e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:16:05 INFO: Loading these models for language: pt (Portuguese):\n",
      "==========================\n",
      "| Processor    | Package |\n",
      "--------------------------\n",
      "| tokenize     | bosque  |\n",
      "| mwt          | bosque  |\n",
      "| pos          | bosque  |\n",
      "| lemma        | bosque  |\n",
      "| constituency | cintil  |\n",
      "| depparse     | bosque  |\n",
      "==========================\n",
      "\n",
      "2023-06-27 16:16:05 INFO: Using device: cuda\n",
      "2023-06-27 16:16:05 INFO: Loading: tokenize\n",
      "2023-06-27 16:16:05 INFO: Loading: mwt\n",
      "2023-06-27 16:16:05 INFO: Loading: pos\n",
      "2023-06-27 16:16:06 INFO: Loading: lemma\n",
      "2023-06-27 16:16:06 INFO: Loading: constituency\n",
      "2023-06-27 16:16:06 INFO: Loading: depparse\n",
      "2023-06-27 16:16:07 INFO: Done loading processors!\n",
      "100%|██████████| 50/50 [07:32<00:00,  9.05s/it]\n",
      "2023-06-27 16:23:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c4ffface64416eaaaaf32f44ab4722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:23:42 INFO: Loading these models for language: de (German):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| pos       | gsd          |\n",
      "| lemma     | gsd          |\n",
      "| depparse  | gsd          |\n",
      "| sentiment | sb10k        |\n",
      "| ner       | germeval2014 |\n",
      "============================\n",
      "\n",
      "2023-06-27 16:23:42 INFO: Using device: cuda\n",
      "2023-06-27 16:23:42 INFO: Loading: tokenize\n",
      "2023-06-27 16:23:42 INFO: Loading: mwt\n",
      "2023-06-27 16:23:42 INFO: Loading: pos\n",
      "2023-06-27 16:23:42 INFO: Loading: lemma\n",
      "2023-06-27 16:23:42 INFO: Loading: depparse\n",
      "2023-06-27 16:23:43 INFO: Loading: sentiment\n",
      "2023-06-27 16:23:43 INFO: Loading: ner\n",
      "2023-06-27 16:23:45 INFO: Done loading processors!\n",
      "100%|██████████| 12/12 [07:42<00:00, 38.54s/it]\n",
      "2023-06-27 16:31:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53d93b9782249018cc74a363cca1909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:31:30 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-06-27 16:31:30 INFO: Using device: cuda\n",
      "2023-06-27 16:31:30 INFO: Loading: tokenize\n",
      "2023-06-27 16:31:30 INFO: Loading: pos\n",
      "2023-06-27 16:31:31 INFO: Loading: lemma\n",
      "2023-06-27 16:31:31 INFO: Loading: constituency\n",
      "2023-06-27 16:31:32 INFO: Loading: depparse\n",
      "2023-06-27 16:31:32 INFO: Loading: sentiment\n",
      "2023-06-27 16:31:33 INFO: Loading: ner\n",
      "2023-06-27 16:31:34 INFO: Done loading processors!\n",
      "100%|██████████| 55/55 [10:36<00:00, 11.57s/it]\n",
      "2023-06-27 16:42:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d442fc75d5844c84a4a2172f51bd4e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:42:12 INFO: Loading these models for language: es (Spanish):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | ancora   |\n",
      "| mwt          | ancora   |\n",
      "| pos          | ancora   |\n",
      "| lemma        | ancora   |\n",
      "| constituency | combined |\n",
      "| depparse     | ancora   |\n",
      "| sentiment    | tass2020 |\n",
      "| ner          | conll02  |\n",
      "===========================\n",
      "\n",
      "2023-06-27 16:42:12 INFO: Using device: cuda\n",
      "2023-06-27 16:42:12 INFO: Loading: tokenize\n",
      "2023-06-27 16:42:12 INFO: Loading: mwt\n",
      "2023-06-27 16:42:12 INFO: Loading: pos\n",
      "2023-06-27 16:42:13 INFO: Loading: lemma\n",
      "2023-06-27 16:42:13 INFO: Loading: constituency\n",
      "2023-06-27 16:42:14 INFO: Loading: depparse\n",
      "2023-06-27 16:42:14 INFO: Loading: sentiment\n",
      "2023-06-27 16:42:15 INFO: Loading: ner\n",
      "2023-06-27 16:42:16 INFO: Done loading processors!\n",
      "100%|██████████| 48/48 [02:15<00:00,  2.83s/it]\n",
      "2023-06-27 16:44:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08540a1cb5a249b18ee1c46cfbe5abad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:44:33 INFO: Loading these models for language: nl (Dutch):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | alpino  |\n",
      "| pos       | alpino  |\n",
      "| lemma     | alpino  |\n",
      "| depparse  | alpino  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2023-06-27 16:44:33 INFO: Using device: cuda\n",
      "2023-06-27 16:44:33 INFO: Loading: tokenize\n",
      "2023-06-27 16:44:33 INFO: Loading: pos\n",
      "2023-06-27 16:44:34 INFO: Loading: lemma\n",
      "2023-06-27 16:44:34 INFO: Loading: depparse\n",
      "2023-06-27 16:44:35 INFO: Loading: ner\n",
      "2023-06-27 16:44:36 INFO: Done loading processors!\n",
      "100%|██████████| 12/12 [07:09<00:00, 35.83s/it]\n",
      "2023-06-27 16:51:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b739841658742e5ad85e1b416f7c95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 16:51:48 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-06-27 16:51:48 INFO: Using device: cuda\n",
      "2023-06-27 16:51:48 INFO: Loading: tokenize\n",
      "2023-06-27 16:51:48 INFO: Loading: pos\n",
      "2023-06-27 16:51:49 INFO: Loading: lemma\n",
      "2023-06-27 16:51:49 INFO: Loading: constituency\n",
      "2023-06-27 16:51:50 INFO: Loading: depparse\n",
      "2023-06-27 16:51:50 INFO: Loading: sentiment\n",
      "2023-06-27 16:51:51 INFO: Loading: ner\n",
      "2023-06-27 16:51:52 INFO: Done loading processors!\n",
      "100%|██████████| 12/12 [08:59<00:00, 44.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for corpus_name, model, tasks in corpus_model_names:\n",
    "\n",
    "    revisions = initialise_df(texts[corpus_name], tasks)\n",
    "    outputs = {task: {idx: [] for idx in texts[corpus_name]} for task in tasks}\n",
    "    tagger = stanza.Pipeline(model, use_gpu=True) \n",
    "\n",
    "    for text_idx, tokens in tqdm(texts[corpus_name].items()):\n",
    "        for i in range(len(tokens)):\n",
    "            partial_input = get_partial_input(tokens, i)\n",
    "            doc = tagger(partial_input)\n",
    "            for task in tasks:\n",
    "                output_tags = get_stanza_outputs(doc, task)\n",
    "                outputs[task][text_idx].append(output_tags)\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            for task in tasks:\n",
    "                revised, conv_revised, effec_revised = get_revised_signal(outputs[task], text_idx, i)                    \n",
    "                identifier = f'text_{text_idx}_token_{i}'\n",
    "                revisions.loc[identifier][f'revision:{task}'] = revised\n",
    "                revisions.loc[identifier][f'convenient-revision:{task}'] = conv_revised\n",
    "                revisions.loc[identifier][f'effective-revision:{task}'] = effec_revised    \n",
    "\n",
    "    for task in tasks:        \n",
    "        with open(PATH_MODEL_OUTPUTS / f'{corpus_name}_{model_family}_{task}.json', 'w') as file:\n",
    "            json.dump(outputs[task], file)\n",
    "\n",
    "    revisions.to_csv(PATH_MODEL_DATA / f'{corpus_name}_{model_family}_revisions.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regressions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
